syntax = "proto3";

package myrtlespeech.protos;

import "google/protobuf/wrappers.proto";


// Constant learning rate.
message ConstantLR {
}


// See PyTorch ``torch.optim.lr_scheduler.StepLR``.
message StepLR {
  // Period of learning rate decay.
  uint32 step_size = 1;

  // Multiplicative factor of learning rate decay.
  google.protobuf.FloatValue gamma = 2;
}


// See PyTorch ``torch.optim.lr_scheduler.ExponentialLR``.
message ExponentialLR {
  // Multiplicative factor of learning rate decay.
  float gamma = 1;
}


// See PyTorch ``torch.optim.lr_scheduler.CosineAnnealingLR``.
message CosineAnnealingLR {
  // Maximum number of iterations.
  uint32 t_max = 1;

  // Minimum learning rate.
  google.protobuf.FloatValue eta_min = 2;
}

// LR linear warmup as described in https://arxiv.org/abs/1910.04209.
message LRWarmup {
  // Number of steps over which warmup is performed
  uint32 num_warmup_steps = 1;

  // Minimum multiple of the initial lr. Defaults to 0.1.
  google.protobuf.FloatValue min_multiple_of_lr = 2;
}

// Polynomial decay learning rate of power 2.
// See ``myrtlespeech.lr_schedule.poly.PolynomialLR``.
message PolynomialLR {
}
