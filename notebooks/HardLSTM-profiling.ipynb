{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # set this before importing torchimport torch \n",
    "import torch \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myrtlespeech.model.hard_lstm import HardLSTM as HardLSTM_ver2\n",
    "from hard import HardLSTM as HardLSTM_ver1\n",
    "\n",
    "from deepspeech_int import HardLSTM as HardLSTM_dsi\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_args(in_size, hidden, seq_len, num_layers, bidirectional, batch, gpu=False):\n",
    "    x = torch.randn(seq_len, batch, in_size)\n",
    "    num_directions = 2 if bidirectional else 1\n",
    "    zeros = torch.zeros(\n",
    "        num_layers * num_directions,\n",
    "        batch,\n",
    "        hidden,\n",
    "        dtype=x.dtype,\n",
    "    )\n",
    "    if gpu:\n",
    "        x = x.cuda()\n",
    "        zeros = zeros.cuda()\n",
    "    return (x, (zeros, zeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check everything runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 100\n",
    "hidden = 128\n",
    "seq_len = 35\n",
    "num_layers = 1\n",
    "bidirectional = False\n",
    "batch = 3\n",
    "\n",
    "args = gen_args(in_size, hidden, seq_len, num_layers, bidirectional, batch)\n",
    "\n",
    "lstm_v1 = HardLSTM_ver1(in_size=in_size, hidden_size=hidden, batch_first=False, bidirectional=bidirectional)\n",
    "lstm_v2 = HardLSTM_ver2(in_size=in_size, hidden_size=hidden, batch_first=False, bidirectional=bidirectional)\n",
    "lstm_v2_script = torch.jit.script(HardLSTM_ver2(in_size=in_size, hidden_size=hidden, batch_first=False, bidirectional=bidirectional))\n",
    "lstm = torch.nn.LSTM(input_size=in_size, hidden_size=hidden, batch_first=False, bidirectional=bidirectional)\n",
    "\n",
    "outputs_v1 = lstm_v1(*args)\n",
    "outputs_v2 = lstm_v2(*args)\n",
    "outputs_v2_script = lstm_v2_script(*args)\n",
    "outputs_n = lstm(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_name = {\n",
    "    0: \"Input Size\",\n",
    "    1: \"Hidden Size\",\n",
    "    2: \"Sequence Length\",\n",
    "    3: \"Number Layers\",\n",
    "    4: \"bidirectional\",\n",
    "    5: \"Batch Size\",\n",
    "}\n",
    "def profile_and_plot(models, dims, construct_each_time = False, batch_first=False, gpu=False):\n",
    "    \"\"\"One and only one of dims is a List. All others are constants.\n",
    "    \n",
    "    dims = (in_size, hidden, seq_len, num_layers, bidirectional, batch)\n",
    "    \n",
    "    \"\"\"\n",
    "    list_seen = False\n",
    "    for idx, dim in enumerate(dims):\n",
    "        if isinstance(dim, List):\n",
    "            assert list_seen == False, \"Only one List can be present\"\n",
    "            list_seen = True\n",
    "            list_idx = idx\n",
    "    assert list_seen == True, \"There must be a List present\"\n",
    "    \n",
    "    values = dims[list_idx]\n",
    "    results = {k : [] for k in models.keys()}\n",
    "    \n",
    "    if not construct_each_time:\n",
    "        lstms = {}\n",
    "        for name, lstm_constr in models.items():\n",
    "            lstm = lstm_constr(dims[0], dims[1], batch_first=batch_first, bidirectional=dims[4])\n",
    "            if gpu:\n",
    "                lstm.cuda()\n",
    "            # warmup\n",
    "            dims_in = copy(dims)\n",
    "            dims_in[list_idx] = values[0]\n",
    "            args = gen_args(*dims_in, gpu=gpu)\n",
    "            lstm(*args)\n",
    "            # add to dict\n",
    "            lstms[name] = lstm \n",
    "    \n",
    "    for value in values:\n",
    "        dims_in = copy(dims)\n",
    "        dims_in[list_idx] = value\n",
    "        args = gen_args(*dims_in, gpu=gpu)\n",
    "\n",
    "        for name, lstm_constr in models.items():\n",
    "            if construct_each_time:\n",
    "                lstm = lstm_constr(dims_in[0], dims_in[1], batch_first=batch_first, bidirectional=dims_in[4])\n",
    "                if gpu:\n",
    "                    lstm.cuda()\n",
    "                # warmup\n",
    "                outputs = lstm(*args)\n",
    "            else:\n",
    "                lstm = lstms[name]\n",
    "\n",
    "            # time\n",
    "            t0 = time.perf_counter() \n",
    "            lstm(*args)\n",
    "            tend = time.perf_counter() \n",
    "            results[name].append((value, tend-t0))\n",
    "            if construct_each_time:\n",
    "                del lstm \n",
    "            \n",
    "    \n",
    "    # plot\n",
    "    for k, res in results.items():\n",
    "        res_ = list(zip(*res))\n",
    "        plt.plot(res_[0], res_[1], label=k)\n",
    "        plt.xlabel(f\"{idx_to_name[list_idx]}\")\n",
    "        plt.ylabel(\"Time /s\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_script_constructor(constructor):\n",
    "    \n",
    "    def cstor(*args, **kwargs):\n",
    "        model = constructor(*args, **kwargs)\n",
    "        return torch.jit.script(model)\n",
    "    return cstor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 100\n",
    "hidden = 256\n",
    "seq_len = 100\n",
    "num_layers = 1\n",
    "bidirectional = False\n",
    "batch = list(range(2, 256, 8))\n",
    "\n",
    "models =  {\"1\": HardLSTM_ver1, \n",
    "           \"2\": HardLSTM_ver2, \n",
    "           \"2_scripted\": get_script_constructor(HardLSTM_ver2),\n",
    "           \"PyTorch\": torch.nn.LSTM}\n",
    "\n",
    "dims = [in_size, hidden, seq_len, num_layers, bidirectional, batch]\n",
    "\n",
    "results = profile_and_plot(models, dims, construct_each_time=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 100\n",
    "hidden = 256\n",
    "seq_len = list(range(2, 800, 30))\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "batch = 16\n",
    "\n",
    "dims = [in_size, hidden, seq_len, num_layers, bidirectional, batch]\n",
    "models =  {\"V1\": HardLSTM_ver1, \n",
    "           \"V2\": HardLSTM_ver2, \n",
    "           \"V2_scripted\": get_script_constructor(HardLSTM_ver2),\n",
    "           \"PyTorch\": torch.nn.LSTM}\n",
    "\n",
    "results = profile_and_plot(models, dims, construct_each_time=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = list(range(1, 512, 32))\n",
    "hidden = 256\n",
    "seq_len = 100\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "batch = 16\n",
    "\n",
    "dims = [in_size, hidden, seq_len, num_layers, bidirectional, batch]\n",
    "models =  {\"V1\": HardLSTM_ver1, \n",
    "           \"V2\": HardLSTM_ver2, \n",
    "           \"V2_scripted\": get_script_constructor(HardLSTM_ver2),\n",
    "           \"PyTorch\": torch.nn.LSTM}\n",
    "\n",
    "results = profile_and_plot(models, dims, construct_each_time=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 100\n",
    "hidden = list(range(2, 1024, 32))\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "batch = 16\n",
    "seq_len = 100\n",
    "\n",
    "dims = [in_size, hidden, seq_len, num_layers, bidirectional, batch]\n",
    "\n",
    "results = profile_and_plot(models, dims, construct_each_time=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, res in results.items():\n",
    "    res_ = list(zip(*res))\n",
    "    plt.plot(res_[0], res_[1], label=k)\n",
    "    plt.xlabel(\"Hidden\")\n",
    "    plt.ylabel(\"time\")\n",
    "    plt.xlim(xmax=500)\n",
    "    plt.ylim(ymax=0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile hard1 and hard2 diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "from myrtlespeech.model.hard_lstm import HardLSTM as HardLSTM_ver2\n",
    "from hard import HardLSTM as HardLSTM_ver1\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # set this before importing torchimport torch \n",
    "import torch \n",
    "import time\n",
    "def gen_args(in_size, hidden, seq_len, num_layers, bidirectional, batch):\n",
    "    x = torch.randn(seq_len, batch, in_size)\n",
    "    num_directions = 2 if bidirectional else 1\n",
    "    zeros = torch.zeros(\n",
    "        num_layers * num_directions,\n",
    "        batch,\n",
    "        hidden,\n",
    "        dtype=x.dtype,\n",
    "    )\n",
    "    return (x, (zeros, zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 100\n",
    "hidden = 512\n",
    "seq_len = 100\n",
    "num_layers = 1\n",
    "bidirectional = False\n",
    "batch = 300\n",
    "dims = in_size, hidden, seq_len, num_layers, bidirectional, batch\n",
    "args = gen_args(*dims)\n",
    "\n",
    "lstm_v1 = HardLSTM_ver1(dims[0], dims[1], batch_first=False, bidirectional=dims[4])\n",
    "lstm_v2 = HardLSTM_ver2(dims[0], dims[1], batch_first=False, bidirectional=dims[4])\n",
    "lstm_v2_script = torch.jit.script(HardLSTM_ver2(dims[0], dims[1], batch_first=False, bidirectional=dims[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('lstm_v1(*args)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('lstm_v2(*args)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('lstm_v2_script(*args)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_plt)",
   "language": "python",
   "name": "conda_plt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
