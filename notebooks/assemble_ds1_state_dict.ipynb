{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.0.weight torch.Size([2048, 494])\n",
      "fc1.0.bias torch.Size([2048])\n",
      "fc2.0.weight torch.Size([2048, 2048])\n",
      "fc2.0.bias torch.Size([2048])\n",
      "fc3.0.weight torch.Size([4096, 2048])\n",
      "fc3.0.bias torch.Size([4096])\n",
      "bi_lstm.rnn.weight_ih_l0 torch.Size([8192, 4096])\n",
      "bi_lstm.rnn.weight_hh_l0 torch.Size([8192, 2048])\n",
      "bi_lstm.rnn.bias_ih_l0 torch.Size([8192])\n",
      "bi_lstm.rnn.bias_hh_l0 torch.Size([8192])\n",
      "bi_lstm.rnn.weight_ih_l0_reverse torch.Size([8192, 4096])\n",
      "bi_lstm.rnn.weight_hh_l0_reverse torch.Size([8192, 2048])\n",
      "bi_lstm.rnn.bias_ih_l0_reverse torch.Size([8192])\n",
      "bi_lstm.rnn.bias_hh_l0_reverse torch.Size([8192])\n",
      "fc4.0.weight torch.Size([2048, 4096])\n",
      "fc4.0.bias torch.Size([2048])\n",
      "out.weight torch.Size([29, 2048])\n",
      "out.bias torch.Size([29])\n"
     ]
    }
   ],
   "source": [
    "## load model\n",
    "from myrtlespeech.protos import task_config_pb2\n",
    "from google.protobuf import text_format\n",
    "from myrtlespeech.builders.task_config import build\n",
    "from myrtlespeech.builders.speech_to_text import build as build_stt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # set this before importing torch\n",
    "import torch\n",
    "from myrtlespeech.model.deep_speech_1 import DeepSpeech1\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "# parse example config file\n",
    "with open(\"../src/myrtlespeech/configs/deep_speech_1_2048_en.config\") as f:\n",
    "    task_config = text_format.Merge(f.read(), task_config_pb2.TaskConfig())\n",
    "\n",
    "stt = build_stt(task_config.speech_to_text)\n",
    "ds1 = stt.model\n",
    "\n",
    "for key, p in ds1.state_dict().items():\n",
    "    print(key, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi_lstm.weight_ih_l0_reverse torch.Size([8192, 4096])\n",
      "fc3.module.0.weight torch.Size([4096, 2048])\n",
      "bi_lstm.bias_ih_l0 torch.Size([8192])\n",
      "fc4.module.0.weight torch.Size([2048, 4096])\n",
      "fc2.module.0.weight torch.Size([2048, 2048])\n",
      "fc1.module.0.weight torch.Size([2048, 494])\n",
      "bi_lstm.bias_hh_l0 torch.Size([8192])\n",
      "bi_lstm.bias_ih_l0_reverse torch.Size([8192])\n",
      "fc4.module.0.bias torch.Size([2048])\n",
      "fc1.module.0.bias torch.Size([2048])\n",
      "bi_lstm.weight_hh_l0 torch.Size([8192, 2048])\n",
      "bi_lstm.weight_ih_l0 torch.Size([8192, 4096])\n",
      "out.module.0.weight torch.Size([29, 2048])\n",
      "bi_lstm.weight_hh_l0_reverse torch.Size([8192, 2048])\n",
      "out.module.0.bias torch.Size([29])\n",
      "fc3.module.0.bias torch.Size([4096])\n",
      "fc2.module.0.bias torch.Size([2048])\n",
      "bi_lstm.bias_hh_l0_reverse torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "state_fp = '/home/julian/models/ds1/96_sparsity.pt'\n",
    "state_dict = torch.load(fp)\n",
    "for key, p in state_dict.items():\n",
    "    print(key, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fc: fc1.module.0.weight -> fc1.0.weight \n",
    "#lstm: bi_lstm.weight_ih_l0 -> bi_lstm.rnn.weight_ih_l0\n",
    "state_dict_ = {}\n",
    "for k, v in state_dict.items():\n",
    "    if 'fc' in k:\n",
    "        new_key = k.replace('.module', '')\n",
    "        state_dict_[new_key] = v\n",
    "    elif 'lstm' in k:\n",
    "        new_key = k.replace('lstm.', 'lstm.rnn.')\n",
    "        state_dict_[new_key] = v\n",
    "    elif 'out' in k:\n",
    "        new_key = k.replace('module.0.', '')\n",
    "        state_dict_[new_key] = v\n",
    "    else:\n",
    "        state_dict_[k] = v\n",
    "\n",
    "ds1.load_state_dict(state_dict_, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save state_dict\n",
    "ds1.eval()\n",
    "\n",
    "torch.save(ds1.state_dict(), '/home/julian/models/ds1/96_sparsity_myrtle.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnx helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "log_dir = '/home/julian/exp/onnx/tmp/'\n",
    "\n",
    "def export_and_check(model, args, fname, input_names, output_names, example_outputs=None, \n",
    "                     dynamic_axes=None, verbose=False, opset_version=11):\n",
    "    fp = Path(log_dir) / fname\n",
    "    model.eval()\n",
    "    \n",
    "    # run model in torch to get expected outputs\n",
    "    exp_outputs = model(*args)\n",
    "    \n",
    "    # export onnx model\n",
    "    torch.onnx.export(model, args, fp, export_params=True, verbose=False,  example_outputs=example_outputs, \n",
    "                      dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names, \n",
    "                      opset_version=opset_version)\n",
    "    \n",
    "    \n",
    "    # Load the ONNX model\n",
    "    model_onnx = onnx.load(fp)\n",
    "    \n",
    "    # Print a human readable representation of the graph\n",
    "    if verbose:\n",
    "        print(\"Printing graph...\")\n",
    "        print(onnx.helper.printable_graph(model_onnx.graph))\n",
    "    \n",
    "    \n",
    "    # Check that the IR is well formed\n",
    "    #onnx.checker.check_model(model_onnx)\n",
    "    \n",
    "    \n",
    "    # onnx runtime\n",
    "    ort_session = ort.InferenceSession(str(fp))\n",
    "    \n",
    "    compare_outputs(ort_session, model, args, output_names, input_names)\n",
    "    \n",
    "    return ort_session\n",
    "\n",
    "def compare_outputs(ort_session, model, args, output_names, input_names):\n",
    "    # convert input args to numpy\n",
    "    exp_outputs = model(*args)\n",
    "    args = [x.numpy() if isinstance(x, torch.Tensor) else x for x in args]\n",
    "    \n",
    "    outputs = ort_session.run(output_names, {k: args[idx] for idx, k in enumerate(input_names)})\n",
    "    \n",
    "    check_outputs_as_expected(outputs, exp_outputs)\n",
    "    \n",
    "    print('model correct!')\n",
    "    \n",
    "\n",
    "    \n",
    "def check_outputs_as_expected(outputs, exp_outputs):\n",
    "    if isinstance(exp_outputs, torch.Tensor):\n",
    "        assert torch.allclose(torch.tensor(outputs), exp_outputs.cpu(), atol=1e-4, rtol=1e-2)\n",
    "    elif isinstance(exp_outputs, tuple) and isinstance(outputs, (tuple, list)):\n",
    "        assert len(exp_outputs) == len(outputs), f\"{len(exp_outputs)} != {len(outputs)}\"\n",
    "        for idx, x in enumerate(outputs):\n",
    "            check_outputs_as_expected(x, exp_outputs[idx])\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected output type(outputs)={type(outputs)} '\n",
    "                         f'with type(exp_outputs)={type(exp_outputs)} ')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and export onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds1_data(seq_len, batch):\n",
    "    # inputs: Tuple: [(batch, channels, in_features, seq_len), (batch,)]\n",
    "    # outputs: Tuple: [(seq_len, batch, out_feat), (batch,)]\n",
    "    seq_len = 2\n",
    "    batch = 5\n",
    "    in_features = 26 * 19\n",
    "    channels = 1\n",
    "\n",
    "    # inputs\n",
    "    inp = torch.randn(batch, channels, in_features, seq_len)\n",
    "    in_lens = torch.randint(low=1, high=seq_len, size=(batch,)).type(torch.int64)\n",
    "    in_lens = in_lens.sort(descending=True)[0]\n",
    "\n",
    "    return(inp, in_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace and Module versions equivalent\n",
      "trace and Module versions equivalent for variable batch\n",
      "model correct!\n",
      "model correct!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq_len = 2\n",
    "batch = 6\n",
    "\n",
    "args = create_ds1_data(seq_len=seq_len, batch=batch)\n",
    "# init onnx params\n",
    "onnx_fname = f'ds1_1024_traced.onnx'\n",
    "input_names = ['input', 'in_lens']\n",
    "output_names = ['output', 'out_lens']\n",
    "dynamic_axes = {'input': {0: 'batch', 3: 'seq_len'}, \n",
    "                     'in_lens': {0: 'batch'},\n",
    "                     'output': {0: 'seq_len', 1: 'batch'},  \n",
    "                     'out_lens': {0: 'batch'}}\n",
    "opset_version = 11\n",
    "log_dir = '/home/julian/exp/onnx/tmp/'\n",
    "fp = Path(log_dir) / onnx_fname\n",
    "    \n",
    "class CollapseArgs(torch.nn.Module):\n",
    "    def __init__(self, submodel):\n",
    "        super().__init__()\n",
    "        self.submodel = submodel\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, lens: torch.Tensor):\n",
    "        return self.submodel((x, lens))\n",
    "\n",
    "# get model\n",
    "model = ds1\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Collapse input args\n",
    "model = CollapseArgs(ds1)\n",
    "\n",
    "# trace model\n",
    "traced = torch.jit.trace(model, args)\n",
    "\n",
    "# script model\n",
    "#scripted = torch.jit.script(model)\n",
    "\n",
    "\n",
    "# expected outputs\n",
    "example_outputs = model(*args)\n",
    "trace_outputs = traced(*args)\n",
    "#script_outputs = scripted(*args)\n",
    "\n",
    "\n",
    "assert torch.allclose(example_outputs[0], trace_outputs[0])\n",
    "assert torch.allclose(example_outputs[1], trace_outputs[1])\n",
    "\n",
    "print(\"trace and Module versions equivalent\")\n",
    "\n",
    "# assert torch.allclose(example_outputs[0], script_outputs[0])\n",
    "# assert torch.allclose(example_outputs[1], script_outputs[1])\n",
    "\n",
    "# print(\"script and Module versions equivalent\")\n",
    "\n",
    "# check they are equivalent with new values\n",
    "seq_len *= 2\n",
    "batch *= 2\n",
    "args = create_ds1_data(seq_len=seq_len, batch=batch)\n",
    "\n",
    "# expected outputs\n",
    "example_outputs = model(*args)\n",
    "trace_outputs = traced(*args)\n",
    "#script_outputs = scripted(*args)\n",
    "\n",
    "\n",
    "assert torch.allclose(example_outputs[0], trace_outputs[0], rtol=1e-2, atol=1e-4)\n",
    "assert torch.allclose(example_outputs[1], trace_outputs[1], rtol=1e-2, atol=1e-4)\n",
    "\n",
    "print(\"trace and Module versions equivalent for variable batch\")\n",
    "\n",
    "# assert torch.allclose(example_outputs[0], script_outputs[0], rtol=1e-2, atol=1e-4)\n",
    "# assert torch.allclose(example_outputs[1], script_outputs[1], rtol=1e-2, atol=1e-4)\n",
    "\n",
    "# print(\"script and Module versions equivalent for variable batch\")\n",
    "\n",
    "\n",
    "model = traced\n",
    "\n",
    "# export onnx model\n",
    "torch.onnx.export(model, args, fp, export_params=True, verbose=False,  example_outputs=example_outputs, \n",
    "                  dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names, \n",
    "                  opset_version=opset_version)\n",
    "\n",
    "    \n",
    "ort_session = export_and_check(model=traced, \n",
    "                 args = args,\n",
    "                 fname = onnx_fname,\n",
    "                 input_names = input_names,\n",
    "                 output_names = output_names,\n",
    "                 dynamic_axes = dynamic_axes,\n",
    "                 opset_version=11,\n",
    "                 example_outputs=example_outputs,\n",
    "    )\n",
    "\n",
    "\n",
    "batch = 10\n",
    "seq_len = 40\n",
    "args = create_ds1_data(seq_len=seq_len, batch=batch)\n",
    "\n",
    "compare_outputs(ort_session, model, args, output_names=output_names, input_names=input_names)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
