{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of longer sequences\n",
    "\n",
    "* This notebook looks at the effect of using longer sequences on WER.  \n",
    "* It includes utils to generate longer files and a `torch.utils.data.Dataset` but this is already generated for `dev-clean` at `brahe:/data/Long/dev-clean`.\n",
    "* It should be possible to set options at the top of this notebook and then `run_all`. \n",
    "* **NOTE:** it is necessary to run this notebook in the conda `myrtlespeech` env. Use commit: `b68aefc25be90ee32151d53366d340aee8fd4026`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # set this before importing torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing import Sequence\n",
    "import fnmatch\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import torchaudio \n",
    "from google.protobuf import text_format\n",
    "from myrtlespeech.protos import task_config_pb2\n",
    "from myrtlespeech.builders.task_config import build\n",
    "from myrtlespeech.run.load import load_seq_to_seq\n",
    "from myrtlespeech.data.dataset.librispeech import LibriSpeech\n",
    "from myrtlespeech.data.alphabet import Alphabet\n",
    "from myrtlespeech.data.dataset.librispeech import LibriSpeech\n",
    "from myrtlespeech.data.batch import seq_to_seq_collate_fn\n",
    "from myrtlespeech.post_process.transducer_beam_decoder import TransducerBeamDecoder\n",
    "from myrtlespeech.post_process.transducer_greedy_decoder import TransducerGreedyDecoder\n",
    "from myrtlespeech.run.run import WordSegmentor\n",
    "from myrtlespeech.post_process.utils import levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPs\n",
    "DATA = Path('/data')\n",
    "SPEAKER = DATA / 'LibriSpeech/SPEAKERS.TXT'\n",
    "LONG_FP = DATA / 'Long'\n",
    "create_files = False # set this to true to re-generate data. (~ 3mins)\n",
    "\n",
    "# Model info\n",
    "log_dir = \"/home/julian/exp/rnnt/2H/1/\" #to load model from\n",
    "epoch = 69 #i.e. state_dict_<epoch>.pt is filename\n",
    "cfg_location = \"../src/myrtlespeech/configs/rnn_t_en_2H.config\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate longer files + transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = Alphabet(list(\" abcdefghijklmnopqrstuvwxyz'_\"))\n",
    "\n",
    "def target_transform(target):\n",
    "        return torch.tensor(\n",
    "            ALPHABET.get_indices(target),\n",
    "            dtype=torch.int32,\n",
    "            requires_grad=False,)\n",
    "            \n",
    "def get_speaker_ids(fp, subset):\n",
    "    ids = []\n",
    "    with open(fp, 'r') as f:\n",
    "        for line in f:\n",
    "            if subset in line:\n",
    "                line = line.split()\n",
    "                line = [l.strip() for l in line]\n",
    "                id_ = int(line[0])\n",
    "                ids.append(id_)\n",
    "    return ids\n",
    "\n",
    "def create_long_files(out_fp, paths, transcriptions, durations, subset='dev-clean'):\n",
    "    \"\"\"Combines all audio/transcript files for a given speaker into a single file.\n",
    "    \n",
    "    Requires ordered paths, transcriptions and durations as input. \n",
    "    \"\"\"\n",
    "    out_fp = out_fp / subset\n",
    "    speaker_ids = get_speaker_ids(SPEAKER, subset)\n",
    "    ids_to_path = {id: [] for id in speaker_ids}\n",
    "    for idx, path in enumerate(paths):\n",
    "        path = Path(path)\n",
    "        fname = path.name\n",
    "        speaker_id = int(path.parents[1].name)\n",
    "        assert speaker_id in speaker_ids\n",
    "        ids_to_path[speaker_id].append((idx, path))\n",
    "    \n",
    "    for id_, paths in ids_to_path.items():\n",
    "        assert paths != [], f\"speaker with id={id_} not present\"\n",
    "        # now generate longer files\n",
    "        all_audio = []\n",
    "        all_transcripts = \"\"\n",
    "        for idx, path in paths:\n",
    "            audio, sr = torchaudio.load(path)\n",
    "            assert sr == 16000\n",
    "            all_audio.append(audio)\n",
    "            all_transcripts += \" \" + transcriptions[idx]\n",
    "        \n",
    "        all_audio = torch.cat(all_audio, dim=1)\n",
    "        \n",
    "        out_dir = out_fp / str(id_)\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # save audio\n",
    "        torchaudio.save(str(out_dir / 'audio.flac'), all_audio, sr)\n",
    "        #save transcripts\n",
    "        with open(out_dir / 'transcript.trans.txt', 'w') as f:\n",
    "            f.write(all_transcripts)\n",
    "            \n",
    "if create_files:\n",
    "    libri = LibriSpeech(root=DATA,\n",
    "                    subsets=['dev-clean'],\n",
    "                    audio_transform= None,\n",
    "                    label_transform=target_transform,\n",
    "                    download=False,\n",
    "                    skip_integrity_check=True,\n",
    "                    max_duration=None,\n",
    "                   )\n",
    "    \n",
    "    create_long_files(LONG_FP, libri.paths, libri.transcriptions, libri.durations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset with new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LongLibrispeech(LibriSpeech):\n",
    "    \"\"\"i.e. inherit methods from LibriSpeech class and ovverride where necessary.\"\"\"\n",
    "    base_dir = \"\"\n",
    "    use_sox = False\n",
    "    def __init__(self, root, subsets, audio_transform, label_transform):\n",
    "        self.root = root\n",
    "        self.subsets = subsets\n",
    "        self._transform = audio_transform\n",
    "        self._target_transform = label_transform\n",
    "        \n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Loads the data from disk.\"\"\"\n",
    "        self.paths: List[str] = []\n",
    "        self.durations: List[float] = []\n",
    "        self.transcriptions: List[str] = []\n",
    "\n",
    "        def raise_(err):\n",
    "            \"\"\"raises error if problem during os.walk\"\"\"\n",
    "            raise err\n",
    "\n",
    "        for subset in self.subsets:\n",
    "            subset_path = os.path.join(self.root, self.base_dir, subset)\n",
    "            for root, dirs, files in os.walk(subset_path, onerror=raise_):\n",
    "                if not files:\n",
    "                    continue\n",
    "                matches = fnmatch.filter(files, \"*.trans.txt\")\n",
    "                assert len(matches) == 1, \"> 1 transcription file found\"\n",
    "                self._parse_transcription_file(root, matches[0])\n",
    "                \n",
    "                # now get audio\n",
    "                matches = fnmatch.filter(files, \"*audio.flac\")\n",
    "                assert len(matches) == 1, f\"> 1 audio file found\"\n",
    "                self._process_audio(root, matches[0]) \n",
    "        self._sort_by_duration()\n",
    "        \n",
    "    def _parse_transcription_file(self, root: str, name: str) -> None:\n",
    "        \"\"\"Parses each sample in a transcription file.\"\"\"\n",
    "        trans_path = os.path.join(root, name)\n",
    "        with open(trans_path, \"r\", encoding=\"utf-8\") as trans:\n",
    "            transcript = trans.read()\n",
    "        transcript = transcript.strip().lower()\n",
    "        self.transcriptions.append(transcript)\n",
    "        \n",
    "    def _process_audio(self, root: str, name: str):\n",
    "        \"\"\"Returns True if sample was dropped due to being too long.\"\"\"\n",
    "        path = os.path.join(root, name)\n",
    "        si, _ = torchaudio.info(path)\n",
    "        duration = (si.length / si.channels) / si.rate\n",
    "        self.paths.append(path)\n",
    "        self.durations.append(duration)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get config and load seq_to_seq\n",
    "\n",
    "with open(cfg_location) as f:\n",
    "    task_config = text_format.Merge(f.read(), task_config_pb2.TaskConfig())\n",
    "\n",
    "task_config.train_config.dataset.librispeech.subset[:] = [0] #dev-clean for speed\n",
    "task_config.eval_config.dataset.librispeech.subset[:] = [0]     #dev-clean for speed\n",
    "seq_to_seq, epochs, _, _ = build(task_config)\n",
    "\n",
    "training_state = {}\n",
    "fp = log_dir + f'state_dict_{epoch}.pt'\n",
    "training_state = load_seq_to_seq(seq_to_seq, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add lengths to audio transform \n",
    "from myrtlespeech.builders.dataset import _add_seq_len\n",
    "audio_transform = _add_seq_len(seq_to_seq.pre_process, len_fn=lambda x: x.size(-1))\n",
    "label_transform = _add_seq_len(target_transform, len_fn=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create audio loader\n",
    "long_libri = LongLibrispeech(root=LONG_FP,\n",
    "                    subsets=['dev-clean'],\n",
    "                    audio_transform= audio_transform,\n",
    "                    label_transform=label_transform,)\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "        dataset=long_libri,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        collate_fn=seq_to_seq_collate_fn,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = seq_to_seq.alphabet\n",
    "WORD_SEGMENTOR = WordSegmentor(\" \")\n",
    "\n",
    "## Can't use fit because the memory usage is too high!\n",
    "def calc_dist(preds, targets, target_lens):\n",
    "    assert len(preds) == 1\n",
    "    for pred, target, target_len in zip(preds, targets, target_lens):\n",
    "        pred_chars = ALPHABET.get_symbols(pred)\n",
    "        exp_chars = ALPHABET.get_symbols(\n",
    "            [int(e) for e in target[:target_len]]\n",
    "        )\n",
    "        pred_words = WORD_SEGMENTOR(pred_chars)\n",
    "        exp_words = WORD_SEGMENTOR(exp_chars)\n",
    "        distance = levenshtein(pred_words, exp_words)\n",
    "        length = len(exp_words)\n",
    "        wer = distance / length\n",
    "        \n",
    "        return distance, length, pred_words, exp_words\n",
    "\n",
    "def eval_sample(batch, decoder):\n",
    "    x, y = batch\n",
    "    predictions = []\n",
    "    wers = []\n",
    "    \n",
    "    preds = decoder(x)\n",
    "    distance, length, pred_words, exp_words = calc_dist(preds, y[0], y[1])\n",
    "    pred = \" \".join(pred_words)\n",
    "    exp = \" \".join(exp_words)\n",
    "    return distance, length, pred, exp\n",
    "\n",
    "    \n",
    "def set_decoder(stt, dec_type, beam_width=None, max_symbols_per_step=100):\n",
    "    if dec_type == 'beam':\n",
    "        assert beam_width is not None\n",
    "        decoder = TransducerBeamDecoder(blank_index=28,\n",
    "                                        beam_width=beam_width,\n",
    "                                        length_norm=False,\n",
    "                                        max_symbols_per_step = max_symbols_per_step,\n",
    "                                        model=stt.model)\n",
    "    elif dec_type == 'greedy':\n",
    "        decoder = TransducerGreedyDecoder(blank_index=28,\n",
    "                                          max_symbols_per_step = max_symbols_per_step,\n",
    "                                          model=stt.model) \n",
    "    else:\n",
    "        raise ValueError()\n",
    "    stt.post_process = decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "exps = []\n",
    "distances = []\n",
    "lengths = []\n",
    "for decoder, beam_width in [('beam', 4)]: # use ('greedy', None) for greedy decoding\n",
    "    print(decoder, beam_width)\n",
    "    seq_to_seq.eval()\n",
    "    set_decoder(seq_to_seq, dec_type=decoder, beam_width=beam_width)\n",
    "    \n",
    "    for loader in [eval_loader,]: \n",
    "        for idx, batch in enumerate(loader):\n",
    "            \n",
    "            distance, length, pred, exp = eval_sample(batch, seq_to_seq.post_process)\n",
    "            distances.append(distance)\n",
    "            lengths.append(length)\n",
    "            preds.append(pred)\n",
    "            exps.append(exp)\n",
    "            try:\n",
    "                print(f'{idx}, wer = {distance / length}')\n",
    "                print(f'running WER = {sum(distances) / sum(lengths)}')\n",
    "            except:\n",
    "                pass\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 16\n",
    "pred = preds[idx]\n",
    "exp = exps[idx] \n",
    "dist = distances[idx]\n",
    "len_ = lengths[idx]\n",
    "print(\"PRED: \", pred, end='\\n\\n')\n",
    "print(\"EXP : \", exp, end='\\n\\n')\n",
    "print(\"leven : \", dist,)\n",
    "print(\"length: \", len_,)\n",
    "print(f\"WER for idx={idx} = {dist/len_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(exp.split()) == len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
