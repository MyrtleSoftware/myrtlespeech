{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transducer Example Usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides example usage of `myrtlespeech` for Transducer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # set this before importing torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "\n",
    "from myrtlespeech.builders.task_config import build\n",
    "from myrtlespeech.run.train import fit\n",
    "from myrtlespeech.protos import task_config_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "\n",
    "from myrtlespeech.run.callbacks.callback import CallbackHandler\n",
    "from myrtlespeech.run.run import TensorBoardLogger, Saver\n",
    "from myrtlespeech.run.callbacks.csv_logger import CSVLogger\n",
    "from myrtlespeech.run.callbacks.callback import Callback, ModelCallback\n",
    "from myrtlespeech.run.callbacks.clip_grad_norm import ClipGradNorm\n",
    "from myrtlespeech.run.callbacks.report_mean_batch_loss import ReportMeanBatchLoss\n",
    "from myrtlespeech.run.callbacks.stop_epoch_after import StopEpochAfter\n",
    "from myrtlespeech.run.callbacks.mixed_precision import MixedPrecision\n",
    "from myrtlespeech.run.run import ClearMemory\n",
    "from myrtlespeech.run.run import ReportTransducerDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = False # since variable size inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/home/julian/experiments/tmp/\"\n",
    "ACC_STEPS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the RNNT model defined in the config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse example config file\n",
    "with open(\"../src/myrtlespeech/configs/rnn_t_en_SMALL.config\") as f:\n",
    "    task_config = text_format.Merge(f.read(), task_config_pb2.TaskConfig())\n",
    "\n",
    "task_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myrtlespeech.builders.speech_to_text import build as build_stt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all components for config\n",
    "# FYI: if using train-clean-100 & dev-clean this cell takes O(60s) \n",
    "seq_to_seq, epochs, train_loader, eval_loader = build(task_config, accumulation_steps=ACC_STEPS)\n",
    "seq_to_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "print(81 * ACC_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe load model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model:\n",
    "    fp = \"/home/user/model/fp/model.pt\"\n",
    "    seq_to_seq.model.load_state_dict(torch.load(fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "* Use callbacks to inject difference into training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback to monitor training and print results\n",
    "class PrintCB(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def on_batch_end(self, **kwargs):\n",
    "        \n",
    "        if self.training and kwargs[\"epoch_minibatches\"] % 100 == 0:\n",
    "            print(kwargs[\"epoch_minibatches\"], kwargs[\"last_loss\"].item())\n",
    "            \n",
    "            return\n",
    "        epoch = kwargs[\"epoch\"]\n",
    "        if kwargs[\"epoch_minibatches\"] % 100 == 0 and kwargs[\"epoch_minibatches\"] != 0:\n",
    "            print(f\"{kwargs['epoch_minibatches']} batches completed\")\n",
    "            try:\n",
    "                wer_reports = kwargs[\"reports\"][seq_to_seq.post_process.__class__.__name__]\n",
    "                wer = wer_reports[\"WER\"]\n",
    "                cer = wer_reports[\"CER\"]\n",
    "                if len(wer_reports[\"transcripts\"]) > 0:\n",
    "                    transcripts = wer_reports[\"transcripts\"][0] #take first element\n",
    "                    pred, exp = transcripts\n",
    "                    pred = \"\".join(pred)\n",
    "                    exp = \"\".join(exp)\n",
    "                    loss = kwargs[\"reports\"][\"ReportMeanBatchLoss\"]\n",
    "                    print(\"batch end, pred: {}, exp: {}, wer: {:.4f}, cer: {:.4f}\".format(pred, exp, wer, cer))\n",
    "\n",
    "            except KeyError:\n",
    "                print(\"no wer - using new decoder?\")\n",
    "        \n",
    "        \n",
    "            \n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        if self.training:\n",
    "            return\n",
    "        epoch = kwargs[\"epoch\"]\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            loss = kwargs[\"reports\"][\"ReportMeanBatchLoss\"]\n",
    "            \n",
    "            wer_reports = kwargs[\"reports\"][seq_to_seq.post_process.__class__.__name__]\n",
    "            wer = wer_reports[\"WER\"]\n",
    "            cer = wer_reports[\"CER\"]\n",
    "            out_str = \"{}, loss: {:.8f}\".format(epoch, loss)\n",
    "            \n",
    "            if len(wer_reports[\"transcripts\"]) > 0:\n",
    "                transcripts = wer_reports[\"transcripts\"][0] #take first element\n",
    "                pred, exp = transcripts\n",
    "                pred = \"\".join(pred)\n",
    "                exp = \"\".join(exp)\n",
    "                \n",
    "                out_str += \", wer: {:.4f}, cer: {:.4f}, pred: {}, exp: {},\".format(wer, cer, pred, exp)\n",
    "            print(out_str)\n",
    "        except KeyError:\n",
    "            \n",
    "            print(\"no wer - using new decoder?\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mixed_precision_cb = MixedPrecision(seq_to_seq) # this can only be initialized once so place it in separate cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnnt_decoder_cb  = ReportTransducerDecoder(seq_to_seq.post_process, seq_to_seq.alphabet, eval_every=20, \n",
    "                                         skip_first_epoch=True)\n",
    "\n",
    "keys_to_log_in_csv = [ \"epoch\", \n",
    "                      f\"reports/{seq_to_seq.post_process.__class__.__name__}/WER\",\n",
    "                      f\"reports/{seq_to_seq.post_process.__class__.__name__}/CER\",\n",
    "                       \"reports/ReportMeanBatchLoss\"]\n",
    "\n",
    "callbacks = [#rnnt_decoder_cb,\n",
    "            ReportMeanBatchLoss(),\n",
    "             \n",
    "            #Note: the following three callbacks, if present, must appear in this order (see docstrings):\n",
    "            TensorBoardLogger(log_dir, seq_to_seq.model, histograms=False),\n",
    "            #mixed_precision_cb,\n",
    "            ClipGradNorm(seq_to_seq, 200),\n",
    "            \n",
    "            # stop training prematurely (useful for debug). \n",
    "            # Ensure following line is commented out to perform full training\n",
    "            StopEpochAfter(epoch_minibatches=10 * ACC_STEPS),\n",
    "            \n",
    "            # logging\n",
    "            #CSVLogger(log_dir + \"log.csv\", keys=keys_to_log_in_csv),\n",
    "            \n",
    "            # save model @ end of epoch:\n",
    "            Saver(log_dir, seq_to_seq.model),\n",
    "            \n",
    "            # The following callback explicitly deletes quanities in callback handler state_dict \n",
    "            # This is useful during transducer training as there are substantial memory pressures\n",
    "            ClearMemory(),\n",
    "            PrintCB()\n",
    "            ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2.24 + 2.09 + 1.98 + 1.49) / 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To test that it is working\n",
    "Check that `grads` is increasing for all steps in accumulation:\n",
    "``grads = zeros_like(p)\n",
    "for _ in accumulation:\n",
    "    loss.backwards()\n",
    "    grads += p.grad.cpu()\n",
    "    print(\"post sum:\", abs(grads).mean().item())\n",
    "    if accumulating:\n",
    "        optim.step()\n",
    "        grads = zeros_like(p)\n",
    "        optim.zero_grad()``\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(\n",
    "    seq_to_seq, \n",
    "    epochs=10,\n",
    "    train_loader=train_loader, \n",
    "    eval_loader=None,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, v in seq_to_seq.named_parameters():\n",
    "    print(n, v.shape, v.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_to_seq = build_stt(task_config.speech_to_text)\n",
    "seq_to_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe eval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = torch.nn.Linear(1, 2)\n",
    "lin.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval = True\n",
    "\n",
    "\n",
    "if run_eval:\n",
    "    eval_cbs = [ReportMeanBatchLoss(), \n",
    "                ReportTransducerDecoder(seq_to_seq.post_process, seq_to_seq.alphabet),\n",
    "                CSVLogger(log_dir + f\"log_eval.csv\", keys=keys_to_log_in_csv)] \n",
    "    \n",
    "    fit(\n",
    "         seq_to_seq, \n",
    "         eval_loader=eval_loader,\n",
    "         callbacks=eval_cbs,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch.nn import Parameter\n",
    "\n",
    "\n",
    "def rnn(rnn, input_size, hidden_size, num_layers, norm=None,\n",
    "        forget_gate_bias=1.0, dropout=0.0, **kwargs):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    if rnn != \"lstm\":\n",
    "        raise ValueError(f\"Unknown rnn={rnn}\")\n",
    "    if norm not in [None, \"batch_norm\", \"layer_norm\"]:\n",
    "        raise ValueError(f\"unknown norm={norm}\")\n",
    "\n",
    "    if rnn == \"lstm\":\n",
    "        if norm is None:\n",
    "            return LstmDrop(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout,\n",
    "                forget_gate_bias=forget_gate_bias,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "        if norm == \"batch_norm\":\n",
    "            return BNRNNSum(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                rnn_layers=num_layers,\n",
    "                batch_norm=True,\n",
    "                dropout=dropout,\n",
    "                forget_gate_bias=forget_gate_bias,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "        if norm == \"layer_norm\":\n",
    "            return torch.jit.script(lnlstm(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout,\n",
    "                forget_gate_bias=forget_gate_bias,\n",
    "                **kwargs\n",
    "            ))\n",
    "\n",
    "\n",
    "class OverLastDim(torch.nn.Module):\n",
    "    \"\"\"Collapses a tensor to 2D, applies a module, and (re-)expands the tensor.\n",
    "    An n-dimensional tensor of shape (s_1, s_2, ..., s_n) is first collapsed to\n",
    "    a tensor with shape (s_1*s_2*...*s_n-1, s_n). The module is called with\n",
    "    this as input producing (s_1*s_2*...*s_n-1, s_n') --- note that the final\n",
    "    dimension can change. This is expanded to (s_1, s_2, ..., s_n-1, s_n') and\n",
    "    returned.\n",
    "    Args:\n",
    "        module (torch.nn.Module): Module to apply. Must accept a 2D tensor as\n",
    "            input and produce a 2D tensor as output, optionally changing the\n",
    "            size of the last dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        *dims, input_size = x.size()\n",
    "\n",
    "        reduced_dims = 1\n",
    "        for dim in dims:\n",
    "            reduced_dims *= dim\n",
    "\n",
    "        x = x.view(reduced_dims, -1)\n",
    "        x = self.module(x)\n",
    "        x = x.view(*dims, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LstmDrop(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, forget_gate_bias,\n",
    "             **kwargs):\n",
    "        \"\"\"Returns an LSTM with forget gate bias init to `forget_gate_bias`.\n",
    "        Args:\n",
    "            input_size: See `torch.nn.LSTM`.\n",
    "            hidden_size: See `torch.nn.LSTM`.\n",
    "            num_layers: See `torch.nn.LSTM`.\n",
    "            dropout: See `torch.nn.LSTM`.\n",
    "            forget_gate_bias: For each layer and each direction, the total value of\n",
    "                to initialise the forget gate bias to.\n",
    "        Returns:\n",
    "            A `torch.nn.LSTM`.\n",
    "        \"\"\"\n",
    "        super(LstmDrop, self).__init__()\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        if forget_gate_bias is not None:\n",
    "            for name, v in self.lstm.named_parameters():\n",
    "                if \"bias_ih\" in name:\n",
    "                    bias = getattr(self.lstm, name)\n",
    "                    bias.data[hidden_size:2*hidden_size].fill_(forget_gate_bias)\n",
    "                if \"bias_hh\" in name:\n",
    "                    bias = getattr(self.lstm, name)\n",
    "                    bias.data[hidden_size:2*hidden_size].fill_(0)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout) if dropout else None\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "\n",
    "        x, h = self.lstm(x, h)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        return x, h\n",
    "\n",
    "\n",
    "\n",
    "class RNNLayer(torch.nn.Module):\n",
    "    \"\"\"A single RNNLayer with optional batch norm.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, rnn_type=torch.nn.LSTM,\n",
    "                 batch_norm=True, forget_gate_bias=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        if batch_norm:\n",
    "            self.bn = OverLastDim(torch.nn.BatchNorm1d(input_size))\n",
    "\n",
    "        if isinstance(rnn_type, torch.nn.LSTM) and not batch_norm:\n",
    "            # batch_norm will apply bias, no need to add a second to LSTM\n",
    "            self.rnn = lstm(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            forget_gate_bias=forget_gate_bias)\n",
    "        else:\n",
    "            self.rnn = rnn_type(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                bias=not batch_norm)\n",
    "\n",
    "    def forward(self, x, hx=None):\n",
    "        if hasattr(self, 'bn'):\n",
    "            x = x.contiguous()\n",
    "            x = self.bn(x)\n",
    "        x, h = self.rnn(x, hx=hx)\n",
    "        return x, h\n",
    "\n",
    "    def _flatten_parameters(self):\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "\n",
    "class BNRNNSum(torch.nn.Module):\n",
    "    \"\"\"RNN wrapper with optional batch norm.\n",
    "    Instantiates an RNN. If it is an LSTM it initialises the forget gate\n",
    "    bias =`lstm_gate_bias`. Optionally applies a batch normalisation layer to\n",
    "    the input with the statistics computed over all time steps.  If dropout > 0\n",
    "    then it is applied to all layer outputs except the last.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, rnn_type=torch.nn.LSTM,\n",
    "                 rnn_layers=1, batch_norm=True, dropout=0.0,\n",
    "                 forget_gate_bias=1.0, norm_first_rnn=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.rnn_layers = rnn_layers\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(rnn_layers):\n",
    "            final_layer = (rnn_layers - 1) == i\n",
    "\n",
    "            self.layers.append(\n",
    "                RNNLayer(\n",
    "                    input_size,\n",
    "                    hidden_size,\n",
    "                    rnn_type=rnn_type,\n",
    "                    batch_norm=batch_norm and (norm_first_rnn or i > 0),\n",
    "                    forget_gate_bias=forget_gate_bias,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if dropout > 0.0 and not final_layer:\n",
    "                self.layers.append(torch.nn.Dropout(dropout))\n",
    "\n",
    "            input_size = hidden_size\n",
    "\n",
    "    def forward(self, x, hx=None):\n",
    "        hx = self._parse_hidden_state(hx)\n",
    "\n",
    "        hs = []\n",
    "        cs = []\n",
    "        rnn_idx = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, torch.nn.Dropout):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x, h_out = layer(x, hx=hx[rnn_idx])\n",
    "                hs.append(h_out[0])\n",
    "                cs.append(h_out[1])\n",
    "                rnn_idx += 1\n",
    "                del h_out\n",
    "\n",
    "        h_0 = torch.stack(hs, dim=0)\n",
    "        c_0 = torch.stack(cs, dim=0)\n",
    "        return x, (h_0, c_0)\n",
    "\n",
    "    def _parse_hidden_state(self, hx):\n",
    "        \"\"\"\n",
    "        Dealing w. hidden state:\n",
    "        Typically in pytorch: (h_0, c_0)\n",
    "            h_0 = ``[num_layers * num_directions, batch, hidden_size]``\n",
    "            c_0 = ``[num_layers * num_directions, batch, hidden_size]``\n",
    "        \"\"\"\n",
    "        if hx is None:\n",
    "            return [None] * self.rnn_layers\n",
    "        else:\n",
    "            h_0, c_0 = hx\n",
    "            assert h_0.shape[0] == self.rnn_layers\n",
    "            return [(h_0[i], c_0[i]) for i in range(h_0.shape[0])]\n",
    "\n",
    "    def _flatten_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (torch.nn.LSTM, torch.nn.GRU, torch.nn.RNN)):\n",
    "                layer._flatten_parameters()\n",
    "\n",
    "\n",
    "class StackTime(torch.nn.Module):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__()\n",
    "        self.factor = int(factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # T, B, U\n",
    "        x, x_lens = x\n",
    "        seq = [x]\n",
    "        for i in range(1, self.factor):\n",
    "            tmp = torch.zeros_like(x)\n",
    "            tmp[:-i, :, :] = x[i:, :, :]\n",
    "            seq.append(tmp)\n",
    "        x_lens = torch.ceil(x_lens.float() / self.factor).int()\n",
    "        return torch.cat(seq, dim=2)[::self.factor, :, :], x_lens\n",
    "\n",
    "\n",
    "def lnlstm(input_size, hidden_size, num_layers, dropout, forget_gate_bias,\n",
    "           **kwargs):\n",
    "    \"\"\"Returns a ScriptModule that mimics a PyTorch native LSTM.\"\"\"\n",
    "    # The following are not implemented.\n",
    "    assert dropout == 0.0\n",
    "\n",
    "    return StackedLSTM(\n",
    "        num_layers,\n",
    "        LSTMLayer,\n",
    "        first_layer_args=[\n",
    "            LayerNormLSTMCell,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            forget_gate_bias,\n",
    "        ],\n",
    "        other_layer_args=[\n",
    "            LayerNormLSTMCell,\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            forget_gate_bias,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "class LSTMLayer(torch.nn.Module):\n",
    "    def __init__(self, cell, *cell_args):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        self.cell = cell(*cell_args)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        state: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        inputs = input.unbind(0)\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        return torch.stack(outputs), state\n",
    "\n",
    "\n",
    "class LayerNormLSTMCell(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, forget_gate_bias):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight_ih = Parameter(torch.randn(4 * hidden_size, input_size))\n",
    "        self.weight_hh = Parameter(torch.randn(4 * hidden_size, hidden_size))\n",
    "\n",
    "        # layernorms provide learnable biases\n",
    "        self.layernorm_i = torch.nn.LayerNorm(4 * hidden_size)\n",
    "        self.layernorm_h = torch.nn.LayerNorm(4 * hidden_size)\n",
    "        self.layernorm_c = torch.nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.layernorm_i.bias.data[hidden_size:2*hidden_size].fill_(0.0)\n",
    "        self.layernorm_h.bias.data[hidden_size:2*hidden_size].fill_(\n",
    "            forget_gate_bias\n",
    "        )\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        state: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        hx, cx = state\n",
    "        igates = self.layernorm_i(torch.mm(input, self.weight_ih.t()))\n",
    "        hgates = self.layernorm_h(torch.mm(hx, self.weight_hh.t()))\n",
    "        gates = igates + hgates\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "\n",
    "        cy = self.layernorm_c((forgetgate * cx) + (ingate * cellgate))\n",
    "        hy = outgate * torch.tanh(cy)\n",
    "\n",
    "        return hy, (hy, cy)\n",
    "\n",
    "\n",
    "def init_stacked_lstm(num_layers, layer, first_layer_args, other_layer_args):\n",
    "    layers = [layer(*first_layer_args)] + [layer(*other_layer_args)\n",
    "                                           for _ in range(num_layers - 1)]\n",
    "    return torch.nn.ModuleList(layers)\n",
    "\n",
    "\n",
    "class StackedLSTM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, layer, first_layer_args, other_layer_args):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.layers: Final[torch.nn.ModuleList] = init_stacked_lstm(\n",
    "            num_layers, layer, first_layer_args, other_layer_args\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        states: Optional[List[Tuple[torch.Tensor, torch.Tensor]]]=None\n",
    "    ) -> Tuple[torch.Tensor, List[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        if states is None:\n",
    "            states: List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
    "            batch = input.size(1)\n",
    "            for layer in self.layers:\n",
    "                states.append(\n",
    "                    (torch.zeros(\n",
    "                        batch,\n",
    "                        layer.cell.hidden_size,\n",
    "                        dtype=input.dtype,\n",
    "                        device=input.device\n",
    "                     ),\n",
    "                     torch.zeros(\n",
    "                         batch,\n",
    "                         layer.cell.hidden_size,\n",
    "                         dtype=input.dtype,\n",
    "                         device=input.device\n",
    "                     )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        output_states: List[Tuple[Tensor, Tensor]] = []\n",
    "        output = input\n",
    "        # XXX: enumerate https://github.com/pytorch/pytorch/issues/14471\n",
    "        i = 0\n",
    "        for rnn_layer in self.layers:\n",
    "            state = states[i]\n",
    "            output, out_state = rnn_layer(output, state)\n",
    "            output_states += [out_state]\n",
    "            i += 1\n",
    "        return output, output_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "fp_out = log_dir + \"model_saved.pt\"\n",
    "if save_model:\n",
    "    torch.save(seq_to_seq.model.state_dict(), fp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
