{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN-T Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log of attempts\n",
    "**Numbers refer to exp_dir:**\n",
    "2. Baseline - took ~ 1000 epochs to almost overfit (WER = 0.2) to x2 samples\n",
    "3. Tried higher lr (0.0003 -> 0.003). This did not help (loss was not able to decrease below 0.7 after 700 epochs). Using lr = 0.0003 going forward.\n",
    "4. NUM_SAMPLES = 4. After x100 epochs - WER = 40% (i.e. distinguishing between samples) but after x200, WER =85%. After 600 epochs, WER=83%. Stopped. \n",
    "5. (4. with no mixed precision. _1346, pred: twenty n, exp: twenty nine, loss 0.00036553, wer: 21.4286_\n",
    "6. Reduced to MUCH smaller model encoder hidden state size = 128, prediction = 64 and lstm type = GRU. Takes much longer to get to resonable loss (hangs around loss=30 for a number of epochs whereas previously this went to < 1 in ~ 10 epochs). Stopped as it doesn't reach < 1 in 500 epochs (loss=1.037)\n",
    "7. Same as 6. but with prediction hidden state = 128. After 500 epochs, loss = 0.88. after 800, loss 0.37256373, wer: 100.0000\n",
    "8. Back to LSTM, otherwise same as 6 (prediction hidden state = 64). MUCH better than 6. After 500 epochs, loss = 0.17, WER = 62%. i.e. previously, WER=100%. After 800, loss: 0.03146841, wer: 58.9286, after 1000, loss: 0.015, wer: 53% . after 2631, loss 0.00012446, wer: 41.0714\n",
    "9. Same as 7 (prediction hidden state = 128) but with LSTM instead of GRU. Results are weird - loss is MUCH better than 8. but WER is much worse. After 500, loss 0.03620639, wer: 98.2143. After 800, loss 0.00908716, wer: 100.0000. BUT @ around 1900 epochs - the loss increases a lot and the WER decreases: After 2010, loss 0.02600078, wer: 25.0000. After 4000, loss 0.00001341, wer: 19.6429. It reached WER=3.5 but then increased again to WER=19%.\n",
    "10. Possibly the lr is too high? use lr = 0.0001, (0.0003 -> 0.0001). Everthing else as in 9. No - this was very slow: After 800, loss 2.69113094, wer: 100.0000. After 3837, loss 0.00392524, wer: 66.0714.\n",
    "11. Back to original lr = 0.0003. Using smaller prediction network (1 layer of size 48). After 500 epochs, loss 0.20005125, wer: 98.2143. i.e. a bit worse than 8. After 800, loss 0.03236475, wer: 82.1429\n",
    "12. Larger model encoder hidden = 400, prediction = 48 (2 layers in both). i.e. these recent expts are taking a v. long time to get to low loss. After 800, loss 0.02806847, wer: 57.1429. After 3073, loss 0.01223648, wer: 48.2143.\n",
    "13. Increase number of samples = 32. It found this v. hard. (Also much slower)\n",
    "14. Repeat 13 but with bidirectional lstm in prediction network. Not sure this actually makes sense for decoding (as it is taking a partial sequence and going forwards and backwards on it). \n",
    "15. Use bi-directional lstm in encoder. This is not allowed in streaming use-case. But is the problem just model capacity? Loss goes down v. slowly. After ~ 600 epochs: loss: 0.16034877, wer: 58.1690, - this is fairly good for 32 samples. \n",
    "16. Higher batch_size and bidirectiona=False. Loss goes down even more slowly (this is not surprising given that we have halved the number of params. After ~600, loss: 0.21108173, wer: 82.7536. After ~800, loss: 0.09715541, wer: 68.2767. The quality of the predictions suggests that the prediction network is still too strong.\n",
    "17. Added dropout. Loss values are lower but *I think* the WER is more stable. After ~600 epochs: loss: 0.97649786, wer: 60.3360. Prediction network appears to be weak. \n",
    "\n",
    "\n",
    "NOTE! - ALL of above experiments where I refer to changing the lr are not correct. i.e. I was not actually changing the lr! So me 'noticing' an effect was just confirmation bias. \n",
    "\n",
    "18. Added larger prediction network hidden_size = 128. Appeared to be better after 2000, loss: 0.47531913, wer: 35.4640,\n",
    "19. Added MelFB preprocessing - the loss went down much quicker in this case. After 400, loss: 0.08690740, wer: 67.0298,\n",
    "20. Added LogMelFB preprocessing. After 400, loss: 0.16579449, wer: 68.2898. After 550: loss: 0.07083742, wer: 56.9760\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/home/julian/exp/speech/myrtlespeech/rnnt/overfit/21/\"\n",
    "NUMBER_SAMPLES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "from google.protobuf import text_format\n",
    "\n",
    "from myrtlespeech.model.rnn_t import RNNTEncoder, RNNT\n",
    "from myrtlespeech.run.run import TensorBoardLogger\n",
    "from myrtlespeech.run.callbacks.csv_logger import CSVLogger\n",
    "from myrtlespeech.run.callbacks.callback import Callback, ModelCallback\n",
    "from myrtlespeech.run.callbacks.clip_grad_norm import ClipGradNorm\n",
    "from myrtlespeech.run.callbacks.report_mean_batch_loss import ReportMeanBatchLoss\n",
    "from myrtlespeech.run.callbacks.stop_epoch_after import StopEpochAfter\n",
    "from myrtlespeech.run.callbacks.mixed_precision import MixedPrecision\n",
    "from myrtlespeech.post_process.utils import levenshtein\n",
    "from myrtlespeech.builders.task_config import build\n",
    "from myrtlespeech.run.train import fit\n",
    "from myrtlespeech.protos import task_config_pb2\n",
    "from myrtlespeech.run.stage import Stage\n",
    "\n",
    "from myrtlespeech.run.train import run_stage\n",
    "from myrtlespeech.run.callbacks.callback import CallbackHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = False\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the RNNT model defined in the config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse example config file\n",
    "with open(\"../src/myrtlespeech/configs/rnn_t_SMALL_en.config\") as f:\n",
    "    task_config = text_format.Merge(f.read(), task_config_pb2.TaskConfig())\n",
    "\n",
    "task_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all components for config\n",
    "# FYI: if using train-clean-100 & dev-clean this cell takes O(60s) \n",
    "seq_to_seq, epochs, train_loader, eval_loader = build(task_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "len(eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "fp_out = log_dir + \"saved_state_dict.pt\"\n",
    "if save_model:\n",
    "    torch.save(seq_to_seq.model.state_dict(), fp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "if load_model:\n",
    "    seq_to_seq.model.load_state_dict(torch.load(fp_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_to_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for p in seq_to_seq.model.parameters():\n",
    "    total += p.numel()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get overfit loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from myrtlespeech.data.batch import seq_to_seq_collate_fn\n",
    "\n",
    "\n",
    "#get dataset from trainloader:\n",
    "dataset = train_loader.sampler.data_source\n",
    "print(dataset)\n",
    "\n",
    "train_loader_no_shuffle = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=seq_to_seq_collate_fn)\n",
    "train_loader_overfit = list(itertools.islice(train_loader_no_shuffle, NUMBER_SAMPLES))\n",
    "eval_loader_overfit = train_loader_overfit #i.e. use the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe change decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myrtlespeech.post_process.rnn_t_beam_decoder import RNNTBeamDecoder\n",
    "from myrtlespeech.post_process.rnn_t_greedy_decoder import RNNTGreedyDecoder\n",
    "use_beam = False\n",
    "no_max = True\n",
    "if use_beam:\n",
    "    decoder = RNNTBeamDecoder(blank_index=28,\n",
    "                                beam_width=4,\n",
    "                                 length_norm=False,\n",
    "                                 max_symbols_per_step = 4,\n",
    "                             model=seq_to_seq.model)\n",
    "else:\n",
    "    decoder = RNNTGreedyDecoder(blank_index=28,\n",
    "                                 max_symbols_per_step = 4,\n",
    "                               model=seq_to_seq.model) \n",
    "\n",
    "seq_to_seq.post_process = decoder\n",
    "\n",
    "if no_max:\n",
    "    seq_to_seq.post_process.max_symbols_per_step = 100\n",
    "else:\n",
    "    seq_to_seq.post_process.max_symbols_per_step = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "* Use callbacks to inject features into training loop. \n",
    "* It is necessary (for now) to use the `RNNTTraining()` callback but the others are optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback to monitor training and print results\n",
    "class PrintCB(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def on_batch_end(self, **kwargs):\n",
    "        \n",
    "        if self.training:\n",
    "            #print(\"training batch ended\")\n",
    "            return\n",
    "        epoch = kwargs[\"epoch\"]\n",
    "        if kwargs[\"epoch_batches\"] % 100 == 0 and kwargs[\"epoch_batches\"] != 0:\n",
    "            print(f\"{kwargs['epoch_batches']} batches completed\")\n",
    "            try:\n",
    "                wer_reports = kwargs[\"reports\"][seq_to_seq.post_process.__class__.__name__]\n",
    "                wer = wer_reports[\"wer\"]\n",
    "                if len(wer_reports[\"transcripts\"]) > 0:\n",
    "                    transcripts = wer_reports[\"transcripts\"][0] #take first element\n",
    "                    pred, exp = transcripts\n",
    "                    pred = \"\".join(pred)\n",
    "                    exp = \"\".join(exp)\n",
    "                    loss = kwargs[\"reports\"][\"ReportMeanBatchLoss\"]\n",
    "                    print(\"batch end, pred: {}, exp: {}, wer: {:.4f}\".format(pred, exp, wer, ))\n",
    "\n",
    "            except KeyError:\n",
    "                print(\"no wer - using new decoder?\")\n",
    "        \n",
    "        \n",
    "            \n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        if self.training:\n",
    "            return\n",
    "        epoch = kwargs[\"epoch\"]\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            loss = kwargs[\"reports\"][\"ReportMeanBatchLoss\"]\n",
    "            \n",
    "            wer_reports = kwargs[\"reports\"][seq_to_seq.post_process.__class__.__name__]\n",
    "            wer = wer_reports[\"wer\"]\n",
    "            \n",
    "            out_str = \"{}, loss: {:.8f}\".format(epoch, loss)\n",
    "            \n",
    "            if len(wer_reports[\"transcripts\"]) > 0:\n",
    "                transcripts = wer_reports[\"transcripts\"][0] #take first element\n",
    "                pred, exp = transcripts\n",
    "                pred = \"\".join(pred)\n",
    "                exp = \"\".join(exp)\n",
    "                \n",
    "                out_str += \", wer: {:.4f}, pred: {}, exp: {},\".format(wer, pred, exp)\n",
    "            print(out_str)\n",
    "        except KeyError:\n",
    "            \n",
    "            print(\"no wer - using new decoder?\")        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myrtlespeech.run.callbacks.rnn_t_training import RNNTTraining\n",
    "from myrtlespeech.run.run import ReportRNNTDecoder\n",
    "\n",
    "rnnt_decoder_cb  = ReportRNNTDecoder(seq_to_seq.post_process, seq_to_seq.alphabet, eval_every=50, \n",
    "                                         skip_first_epoch=True)\n",
    "\n",
    "\n",
    "keys_to_log = [\"epoch\", \n",
    "        f\"reports/{seq_to_seq.post_process.__class__.__name__}/wer\",\n",
    "        \"reports/ReportMeanBatchLoss\"]\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [RNNTTraining(),\n",
    "            ReportMeanBatchLoss(), \n",
    "            TensorBoardLogger(log_dir, seq_to_seq.model, histograms=True),\n",
    "            MixedPrecision(seq_to_seq),\n",
    "            ClipGradNorm(seq_to_seq, 400),\n",
    "            rnnt_decoder_cb,\n",
    "             \n",
    "            #stop prematurely (useful for debug). Ensure following line is commented out to perform full training\n",
    "            #StopEpochAfter(epoch_batches=2),\n",
    "            \n",
    "            # logging\n",
    "            CSVLogger(log_dir + \"log.csv\", keys=keys_to_log),\n",
    "            \n",
    "            PrintCB(),\n",
    "            Saver(log_dir, seq_to_seq.model)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_lr = False\n",
    "\n",
    "for param_group in seq_to_seq.optim.param_groups:\n",
    "    print(\"current lr: \", param_group['lr'])\n",
    "\n",
    "new_lr = 0.0001\n",
    "if change_lr:\n",
    "    for param_group in seq_to_seq.optim.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "for param_group in seq_to_seq.optim.param_groups:\n",
    "    print(\"new lr: \", param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(\n",
    "    seq_to_seq, \n",
    "    epochs=40,\n",
    "    train_loader=train_loader_overfit, \n",
    "    eval_loader=eval_loader_overfit,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run eval?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval = True\n",
    "eval_cbs = None\n",
    "if run_eval:\n",
    "    eval_cbs = [RNNTTraining(),\n",
    "            ReportMeanBatchLoss(), \n",
    "            ReportRNNTDecoder(seq_to_seq.post_process, seq_to_seq.alphabet),\n",
    "            CSVLogger(log_dir + \"log_eval.csv\", keys=keys_to_log),\n",
    "            TensorBoardLogger(log_dir, seq_to_seq.model),\n",
    "            PrintCB(),] \n",
    "    cb_handler = CallbackHandler(eval_cbs, False)\n",
    "    cb_handler.on_train_begin(epochs=2)\n",
    "    \n",
    "    run_stage(seq_to_seq, cb_handler, eval_loader, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(\n",
    "    seq_to_seq: SeqToSeq,\n",
    "    eval_loader: DataLoader,\n",
    "    callbacks: Optional[Collection[Callback]] = None,\n",
    ") -> None:\n",
    "    is_training = False\n",
    "    cb_handler = CallbackHandler(callbacks, is_training)\n",
    "    cb_handler.on_train_begin(epochs=1)\n",
    "    \n",
    "    run_stage(seq_to_seq, cb_handler, eval_loader, is_training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(\n",
    "    seq_to_seq, \n",
    "    eval_loader=eval_loader_overfit,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
