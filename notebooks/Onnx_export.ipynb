{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # set this before importing torch\n",
    "import torch\n",
    "import onnx\n",
    "\n",
    "\n",
    "from myrtlespeech.protos import task_config_pb2\n",
    "from google.protobuf import text_format\n",
    "from myrtlespeech.builders.task_config import build\n",
    "from myrtlespeech.run.run import Saver\n",
    "from myrtlespeech.model.fully_connected import FullyConnected\n",
    "from myrtlespeech.model.rnn import RNN\n",
    "\n",
    "from myrtlespeech.run.train import fit\n",
    "from myrtlespeech.run.load import load_seq_to_seq\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import onnx\n",
    "\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnt_log_dir = '/home/julian/exp/rnnt/wer_down/2L/2/'\n",
    "log_dir = '/home/julian/exp/onnx/tmp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test to check identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_and_check(model, args, fname, input_names, output_names, example_outputs=None, \n",
    "                     dynamic_axes=None, verbose=False, opset_version=9):\n",
    "    fp = Path(log_dir) / fname\n",
    "    model.eval()\n",
    "    \n",
    "    # run model in torch to get expected outputs\n",
    "    exp_outputs = model(*args)\n",
    "    \n",
    "    # export onnx model\n",
    "    torch.onnx.export(model, args, fp, export_params=True, verbose=False,  example_outputs=None, \n",
    "                      dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names, \n",
    "                      opset_version=opset_version)\n",
    "    \n",
    "    \n",
    "    # Load the ONNX model\n",
    "    model_onnx = onnx.load(fp)\n",
    "    \n",
    "    # Print a human readable representation of the graph\n",
    "    if verbose:\n",
    "        print(\"Printing graph...\")\n",
    "        print(onnx.helper.printable_graph(model_onnx.graph))\n",
    "    \n",
    "    # Check that the IR is well formed\n",
    "    onnx.checker.check_model(model_onnx)\n",
    "    \n",
    "    # onnx runtime\n",
    "    ort_session = ort.InferenceSession(str(fp))\n",
    "    \n",
    "    # convert input args to numpy\n",
    "    args = [x.numpy() if isinstance(x, torch.Tensor) else x for x in args]\n",
    "    \n",
    "    outputs = ort_session.run(output_names, {k: args[idx] for idx, k in enumerate(input_names)})\n",
    "    \n",
    "    check_outputs_as_expected(outputs, exp_outputs)\n",
    "    \n",
    "    print('model correct!')\n",
    "\n",
    "def check_outputs_as_expected(outputs, exp_outputs):\n",
    "    if isinstance(exp_outputs, torch.Tensor):\n",
    "        assert torch.allclose(torch.tensor(outputs), exp_outputs.cpu())\n",
    "    elif isinstance(exp_outputs, tuple) and isinstance(outputs, (tuple, list)):\n",
    "        assert len(exp_outputs) == len(outputs), f\"{len(exp_outputs)} != {len(outputs)}\"\n",
    "        for idx, x in enumerate(outputs):\n",
    "            check_outputs_as_expected(x, exp_outputs[idx])\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected output type(outputs)={type(outputs)} '\n",
    "                         f'with type(exp_outputs)={type(exp_outputs)} ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wrapper to unwrap tuple args\n",
    "class CollapseTupleArgs(torch.nn.Module):\n",
    "    def __init__(self, submodel):\n",
    "        super().__init__()\n",
    "        self.submodel = submodel\n",
    "    def forward(self, *args):\n",
    "        return self.submodel(args)\n",
    "\n",
    "class FlattenTupleArgs(torch.nn.Module):\n",
    "    \"\"\"Flatten Tuple Args before returning.\"\"\"\n",
    "    def __init__(self, submodel):\n",
    "        super().__init__()\n",
    "        self.submodel = submodel\n",
    "    def forward(self, *args):\n",
    "        res = self.submodel(*args)\n",
    "        ret = []\n",
    "        if isinstance(res, tuple):\n",
    "            for x in res:\n",
    "                if isinstance(x, tuple):\n",
    "                    for y in x:\n",
    "                        ret.append(y)\n",
    "                else:\n",
    "                    ret.append(x)\n",
    "        return tuple(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_and_check(model=torch.nn.Linear(2, 3), \n",
    "                 args = (torch.randn(5, 2),),\n",
    "                 fname = 'linear.onnx',\n",
    "                 input_names = ['input'],\n",
    "                 output_names = ['output'],\n",
    "                 dynamic_axes = {'input': {0: 'batch'}, 'output': {0: 'batch'}},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (seq_len, batch, input_size)\n",
    "seq_len = 2\n",
    "batch = 1\n",
    "input_size = 2\n",
    "num_layers = 1\n",
    "export_and_check(model=torch.nn.RNN(input_size, 3, num_layers), \n",
    "                 args = (torch.randn(seq_len, batch, input_size),),\n",
    "                 fname = 'rnn.onnx',\n",
    "                 input_names = ['input'],\n",
    "                 output_names = ['output_1', 'output_2'],\n",
    "                 dynamic_axes = {'input': {0: 'seq_len', 1: 'batch'}, \n",
    "                                 'output_1': {0: 'seq_len', 1: 'batch'},  \n",
    "                                 'output_2': {1: 'batch'}},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "seq_len = 2\n",
    "batch = 1\n",
    "input_size = 2\n",
    "num_layers = 1\n",
    "lstm = FlattenTupleArgs(torch.nn.LSTM(input_size, 3, num_layers))\n",
    "export_and_check(model=lstm, \n",
    "                 args = (torch.randn(seq_len, batch, input_size),),\n",
    "                 fname = 'lstm.onnx',\n",
    "                 input_names = ['input'],\n",
    "                 output_names = ['output_1', 'output_2', 'output_3'],\n",
    "                 dynamic_axes = {'input': {0: 'seq_len', 1: 'batch'}, \n",
    "                                 'output_1': {0: 'seq_len', 1: 'batch'},  \n",
    "                                 'output_2': {1: 'batch'},\n",
    "                                 'output_3': {1: 'batch'}},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## myrtlespeech submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test myrtlespeech fully_connected\n",
    "# args: ([batch, seq_len, in_features], (batch,))\n",
    "\n",
    "seq_len = 2\n",
    "batch = 1\n",
    "in_features = 5\n",
    "fc = CollapseTupleArgs(FullyConnected(in_features, out_features=3, \n",
    "                                    num_hidden_layers=2,  hidden_size = 2, hidden_activation_fn=torch.nn.ReLU()),)\n",
    "\n",
    "inp = torch.randn(batch, seq_len, in_features)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,))\n",
    "args = (inp, in_lens)\n",
    "\n",
    "export_and_check(model=fc, \n",
    "                 args = args,\n",
    "                 fname = 'fc_myrtlespeech.onnx',\n",
    "                 input_names = ['input', 'in_lens'],\n",
    "                 output_names = ['output', 'out_lens'],\n",
    "                 dynamic_axes = {'input': {0: 'batch', 1: 'seq_len'}, \n",
    "                                 'in_lens': {0: 'batch'},\n",
    "                                 'output': {0: 'batch', 1: 'seq_len'}, \n",
    "                                 'out_lens': {0: 'batch'}},                        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from typing import TypeVar\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class RNNType(IntEnum):\n",
    "    LSTM = 0\n",
    "    GRU = 1\n",
    "    BASIC_RNN = 2\n",
    "\n",
    "\n",
    "RNNState = TypeVar(\"RNNState\", torch.Tensor, Tuple[torch.Tensor, torch.Tensor])\n",
    "\n",
    "RNNLengths = TypeVar(\"RNNLengths\", bound=torch.Tensor)\n",
    "\n",
    "RNNInput = Union[torch.Tensor, Tuple[torch.Tensor, Optional[RNNState]]]\n",
    "\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    \"\"\"A recurrent neural network.\n",
    "\n",
    "    See :py:class:`torch.nn.LSTM`, :py:class:`torch.nn.GRU` and\n",
    "    :py:class:`torch.nn.RNN` for more information as these are used internally\n",
    "    (see Attributes).\n",
    "\n",
    "    This wrapper ensures the sequence length information is correctly used by\n",
    "    the RNN (i.e. using :py:func:`torch.nn.utils.rnn.pad_packed_sequence` and\n",
    "    :py:func:`torch.nn.utils.rnn.pad_packed_sequence`).\n",
    "\n",
    "    Args:\n",
    "        rnn_type: The type of recurrent neural network cell to use. See\n",
    "            :py:class:`RNNType` for a list of the supported types.\n",
    "\n",
    "        input_size: The number of features in the input.\n",
    "\n",
    "        hidden_size: The number of features in the hidden state.\n",
    "\n",
    "        num_layers: The number of recurrent layers.\n",
    "\n",
    "        bias: If :py:data:`False`, then the layer does not use the bias weights\n",
    "            ``b_ih`` and ``b_hh``.\n",
    "\n",
    "        dropout: If non-zero, introduces a dropout layer on the\n",
    "            outputs of each LSTM layer except the last layer,\n",
    "            with dropout probability equal to ``dropout``.\n",
    "\n",
    "        bidirectional: If :py:data:`True`, becomes a bidirectional LSTM.\n",
    "\n",
    "        forget_gate_bias: If ``rnn_type == RNNType.LSTM`` and ``bias = True``\n",
    "            then the sum of forget gate bias after initialisation equals this\n",
    "            value if it is not :py:data:`None`. If it is :py:data:`None` then\n",
    "            the default initialisation is used.\n",
    "\n",
    "            See `Jozefowicz et al., 2015\n",
    "            <http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf>`_.\n",
    "\n",
    "        batch_first: If :py:data:`True`, then the input and output tensors are\n",
    "            provided as ``[batch, seq_len, in_features]``.\n",
    "\n",
    "    Attributes:\n",
    "        rnn: A :py:class:`torch.LSTM`, :py:class:`torch.GRU`, or\n",
    "            :py:class:`torch.RNN` instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type: RNNType,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int = 1,\n",
    "        bias: int = True,\n",
    "        dropout: float = 0.0,\n",
    "        bidirectional: bool = False,\n",
    "        forget_gate_bias: Optional[float] = None,\n",
    "        batch_first: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if rnn_type == RNNType.LSTM:\n",
    "            rnn_cls = torch.nn.LSTM\n",
    "        elif rnn_type == RNNType.GRU:\n",
    "            rnn_cls = torch.nn.GRU\n",
    "        elif rnn_type == RNNType.BASIC_RNN:\n",
    "            rnn_cls = torch.nn.RNN\n",
    "        else:\n",
    "            raise ValueError(f\"unknown rnn_type {rnn_type}\")\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bias=bias,\n",
    "            batch_first=self.batch_first,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        if rnn_type == RNNType.LSTM and bias and forget_gate_bias is not None:\n",
    "            for l in range(num_layers):\n",
    "                ih = getattr(self.rnn, f\"bias_ih_l{l}\")\n",
    "                ih.data[hidden_size : 2 * hidden_size] = forget_gate_bias\n",
    "                hh = getattr(self.rnn, f\"bias_hh_l{l}\")\n",
    "                hh.data[hidden_size : 2 * hidden_size] = 0.0\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.rnn = self.rnn.cuda()\n",
    "\n",
    "    def forward(\n",
    "        self, x: Tuple[Union[torch.Tensor, Tuple[torch.Tensor, Optional[RNNState]]], RNNLengths]\n",
    "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor, Optional[RNNState]]], RNNLengths]:\n",
    "        r\"\"\"Returns the result of applying the rnn to ``x[0]``.\n",
    "\n",
    "        All inputs are moved to the GPU with :py:meth:`torch.nn.Module.cuda`\n",
    "        if :py:func:`torch.cuda.is_available` was :py:data:`True` on\n",
    "        initialisation.\n",
    "\n",
    "        Args:\n",
    "            x: A Tuple ``(x[0], x[1])``. ``x[0]`` can take two forms: either it\n",
    "                is a tuple ``x[0] = (inp, hid)`` or it is a torch tensor\n",
    "                ``x[0] = inp``. ``inp`` is the network input (a\n",
    "                :py:class:`torch.Tensor`) with size ``[seq_len, batch,\n",
    "                in_features]``. ``hid`` is the RNN hidden state which is\n",
    "                either a length 2 Tuple of :py:class:`torch.Tensor`s or\n",
    "                a single :py:class:`torch.Tensor` depending on the ``RNNType``\n",
    "                (see :py:class:`torch.nn` documentation for more information).\n",
    "\n",
    "                The return type `res[0]` will be the same as the `x[0]` type so\n",
    "                you should pass `hid = None` if you would like the hidden state\n",
    "                returned and it is the start-of-sequence. In this case, the\n",
    "                hidden state(s) will be initialised to zero in PyTorch.\n",
    "\n",
    "                ``x[1]`` is a :py:class:`torch.Tensor` where each entry\n",
    "                represents the sequence length of the corresponding network\n",
    "                *input* sequence.\n",
    "\n",
    "        Returns:\n",
    "            A Tuple ``(res[0], res[1])``. ``res[0]`` will take the same form as\n",
    "            ``x[0]``: either a tuple ``res[0] = (out, hid)`` or a\n",
    "            :py:class:`torch.Tensor``. ``res[0] = out``. ``out`` is the\n",
    "            result after applying the RNN to ``inp``. It will have size\n",
    "            ``[seq_len, batch, out_features]``. ``hid`` is the\n",
    "            returned RNN hidden state which is either a length 2 Tuple of\n",
    "            :py:class:`torch.Tensor`s or a single :py:class:`torch.Tensor`\n",
    "            depending on the ``RNNType`` (see :py:class:`torch.nn`\n",
    "            documentation for more information).\n",
    "\n",
    "            ``res[1]`` is a :py:class:`torch.Tensor` where each entry\n",
    "            represents the sequence length of the corresponding network\n",
    "            *output* sequence. This will be equal to ``x[1]`` as this layer\n",
    "            does not change sequence length.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(x[0], torch.Tensor):\n",
    "            inp = x[0]\n",
    "            hid = None\n",
    "            return_tuple = False\n",
    "        elif isinstance(x[0], tuple) and len(x[0]) == 2:\n",
    "            inp, hid = x[0]\n",
    "            return_tuple = True\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"`x[0]` must be of form (input, hidden) or (input).\"\n",
    "            )\n",
    "\n",
    "        if self.use_cuda:\n",
    "            inp = inp.cuda()\n",
    "            if hid is not None:\n",
    "                if isinstance(hid, tuple) and len(hid) == 2:  # LSTM\n",
    "                    hid = hid[0].cuda(), hid[1].cuda()\n",
    "                elif isinstance(hid, torch.Tensor):  # Vanilla RNN/GRU\n",
    "                    hid = hid.cuda()\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"hid must be a length 2 tuple or a torch.Tensor.\"\n",
    "                    )\n",
    "\n",
    "        # Record sequence length to enable DataParallel\n",
    "        # https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism\n",
    "        total_length = inp.size(0 if not self.batch_first else 1)\n",
    "        inp = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            input=inp,\n",
    "            lengths=x[1],\n",
    "            batch_first=self.batch_first,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        out, hid = self.rnn(inp, hx=hid)\n",
    "\n",
    "        out, lengths = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=out,\n",
    "            batch_first=self.batch_first,\n",
    "            total_length=total_length,\n",
    "        )\n",
    "\n",
    "        if return_tuple:\n",
    "            return (out, hid), lengths\n",
    "        return out, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test myrtlespeech RNN\n",
    "# args: ([seq_len, batch, in_features], (batch,))\n",
    "\n",
    "seq_len = 2\n",
    "batch = 2\n",
    "input_size = 2\n",
    "num_layers = 1\n",
    "hidden_size = 3\n",
    "rnn = RNN(rnn_type=2, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "rnn = CollapseTupleArgs(rnn)\n",
    "inp = torch.randn(seq_len, batch, input_size)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,))\n",
    "args = (inp, in_lens)\n",
    "\n",
    "export_and_check(model=rnn, \n",
    "                 args = args,\n",
    "                 fname = 'fc_myrtlespeech.onnx',\n",
    "                 input_names = ['input', 'in_lens'],\n",
    "                 output_names = ['output', 'out_lens'],\n",
    "                 dynamic_axes = {'input': {1: 'batch', 0: 'seq_len'}, \n",
    "                                 'in_lens': {0: 'batch'},\n",
    "                                 'output': {1: 'batch', 0: 'seq_len'}, \n",
    "                                 'out_lens': {0: 'batch'}},\n",
    "                 opset_version=11,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myrtlespeech.builders.speech_to_text import build as build_stt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse example config file\n",
    "with open(\"../src/myrtlespeech/configs/deep_speech_1_en.config\") as f:\n",
    "    task_config = text_format.Merge(f.read(), task_config_pb2.TaskConfig())\n",
    "\n",
    "stt = build_stt(task_config.speech_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = CollapseTupleArgs(stt.model)\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: Tuple: [(batch, channels, in_features, seq_len), (batch,)]\n",
    "# outputs: Tuple: [(seq_len, batch, out_feat), (batch,)]\n",
    "seq_len = 2\n",
    "batch = 1\n",
    "in_features = 26 * 19\n",
    "num_layers = 1\n",
    "channels = 1\n",
    "\n",
    "\n",
    "# generate args\n",
    "\n",
    "inp = torch.randn(batch, channels, in_features, seq_len)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,))\n",
    "args = (inp, in_lens)\n",
    "export_and_check(model=ds1, \n",
    "                 args = args,\n",
    "                 fname = 'ds1_1024.onnx',\n",
    "                 input_names = ['input', 'in_lens'],\n",
    "                 output_names = ['output', 'out_lens'],\n",
    "                 dynamic_axes = {'input': {0: 'batch', 3: 'seq_len'}, \n",
    "                                 'in_lens': {0: 'batch'},\n",
    "                                 'output': {0: 'batch', 3: 'seq_len'}, \n",
    "                                 'output': {0: 'seq_len', 1: 'batch'},  \n",
    "                                 'out_lens': {0: 'batch'}},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse example config file\n",
    "with open(\"../src/myrtlespeech/configs/rnn_t_en_2L.config\") as f:\n",
    "    task_config = text_format.Merge(f.read(), task_config_pb2.TaskConfig())\n",
    "\n",
    "task_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/home/julian/exp/rnnt/wer_down/2L/2/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all components for config\n",
    "# FYI: if using train-clean-100 & dev-clean this cell takes O(60s) \n",
    "seq_to_seq, epochs, train_loader, eval_loader = build(task_config, accumulation_steps=2)\n",
    "seq_to_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "epoch = 68\n",
    "training_state = {}\n",
    "if load_model:\n",
    "    fp = log_dir + f'state_dict_{epoch}.pt'\n",
    "    #fp = '/home/julian/exp/rnnt/wer_down/2D/1/model_saved.pt'\n",
    "    training_state = load_seq_to_seq(seq_to_seq, fp)\n",
    "    \n",
    "seq_to_seq.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
