{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "log_dir = '/home/julian/exp/onnx/tmp/'\n",
    "\n",
    "def export_and_check(model, args, fname, input_names, output_names, example_outputs=None, \n",
    "                     dynamic_axes=None, verbose=False, opset_version=11):\n",
    "    fp = Path(log_dir) / fname\n",
    "    model.eval()\n",
    "    \n",
    "    # run model in torch to get expected outputs\n",
    "    exp_outputs = model(*args)\n",
    "    \n",
    "    # export onnx model\n",
    "    torch.onnx.export(model, args, fp, export_params=True, verbose=False,  example_outputs=example_outputs, \n",
    "                      dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names, \n",
    "                      opset_version=opset_version)\n",
    "    \n",
    "    \n",
    "    # Load the ONNX model\n",
    "    model_onnx = onnx.load(fp)\n",
    "    \n",
    "    # Print a human readable representation of the graph\n",
    "    if verbose:\n",
    "        print(\"Printing graph...\")\n",
    "        print(onnx.helper.printable_graph(model_onnx.graph))\n",
    "    \n",
    "    \n",
    "    # Check that the IR is well formed\n",
    "    #onnx.checker.check_model(model_onnx)\n",
    "    \n",
    "    \n",
    "    # onnx runtime\n",
    "    ort_session = ort.InferenceSession(str(fp))\n",
    "    \n",
    "    compare_outputs(ort_session, model, args, output_names, input_names)\n",
    "    \n",
    "    return ort_session\n",
    "\n",
    "def compare_outputs(ort_session, model, args, output_names, input_names):\n",
    "    # convert input args to numpy\n",
    "    exp_outputs = model(*args)\n",
    "    args = [x.numpy() if isinstance(x, torch.Tensor) else x for x in args]\n",
    "    \n",
    "    outputs = ort_session.run(output_names, {k: args[idx] for idx, k in enumerate(input_names)})\n",
    "    \n",
    "    check_outputs_as_expected(outputs, exp_outputs)\n",
    "    \n",
    "    print('model correct!')\n",
    "    \n",
    "\n",
    "    \n",
    "def check_outputs_as_expected(outputs, exp_outputs):\n",
    "    if isinstance(exp_outputs, torch.Tensor):\n",
    "        assert torch.allclose(torch.tensor(outputs), exp_outputs.cpu(), atol=1e-4, rtol=1e-2)\n",
    "    elif isinstance(exp_outputs, tuple) and isinstance(outputs, (tuple, list)):\n",
    "        assert len(exp_outputs) == len(outputs), f\"{len(exp_outputs)} != {len(outputs)}\"\n",
    "        for idx, x in enumerate(outputs):\n",
    "            check_outputs_as_expected(x, exp_outputs[idx])\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected output type(outputs)={type(outputs)} '\n",
    "                         f'with type(exp_outputs)={type(exp_outputs)} ')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace and Module versions equivalent\n",
      "trace and Module versions equivalent for variable batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/onnx/symbolic_opset9.py:1436: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable lenght with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  \"or define the initial states (h0/c0) as inputs of the model. \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model correct!\n",
      "model correct!\n"
     ]
    }
   ],
   "source": [
    "from myrtlespeech.protos import task_config_pb2\n",
    "from google.protobuf import text_format\n",
    "from myrtlespeech.builders.task_config import build\n",
    "from myrtlespeech.builders.speech_to_text import build as build_stt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # set this before importing torch\n",
    "import torch\n",
    "from myrtlespeech.model.deep_speech_1 import DeepSpeech1\n",
    "from pathlib import Path\n",
    "\n",
    "# parse example config file\n",
    "with open(\"../src/myrtlespeech/configs/deep_speech_1_en.config\") as f:\n",
    "    task_config = text_format.Merge(f.read(), task_config_pb2.TaskConfig())\n",
    "\n",
    "stt = build_stt(task_config.speech_to_text)\n",
    "ds1 = stt.model\n",
    "\n",
    "# inputs: Tuple: [(batch, channels, in_features, seq_len), (batch,)]\n",
    "# outputs: Tuple: [(seq_len, batch, out_feat), (batch,)]\n",
    "seq_len = 2\n",
    "batch = 5\n",
    "in_features = 26 * 19\n",
    "channels = 1\n",
    "\n",
    "\n",
    "\n",
    "# inputs\n",
    "inp = torch.randn(batch, channels, in_features, seq_len)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,)).type(torch.int64)\n",
    "in_lens = in_lens.sort(descending=True)[0]\n",
    "\n",
    "args = (inp, in_lens)\n",
    "\n",
    "\n",
    "# init onnx params\n",
    "onnx_fname = f'ds1_1024_traced.onnx'\n",
    "input_names = ['input', 'in_lens']\n",
    "output_names = ['output', 'out_lens']\n",
    "dynamic_axes = {'input': {0: 'batch', 3: 'seq_len'}, \n",
    "                     'in_lens': {0: 'batch'},\n",
    "                     'output': {0: 'seq_len', 1: 'batch'},  \n",
    "                     'out_lens': {0: 'batch'}}\n",
    "opset_version = 11\n",
    "log_dir = '/home/julian/exp/onnx/tmp/'\n",
    "fp = Path(log_dir) / onnx_fname\n",
    "    \n",
    "class CollapseArgs(torch.nn.Module):\n",
    "    def __init__(self, submodel):\n",
    "        super().__init__()\n",
    "        self.submodel = submodel\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, lens: torch.Tensor):\n",
    "        return self.submodel((x, lens))\n",
    "\n",
    "# get model\n",
    "model = ds1\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Collapse input args\n",
    "model = CollapseArgs(ds1)\n",
    "\n",
    "# trace model\n",
    "traced = torch.jit.trace(model, args)\n",
    "\n",
    "# script model\n",
    "#scripted = torch.jit.script(model)\n",
    "\n",
    "\n",
    "# expected outputs\n",
    "example_outputs = model(*args)\n",
    "trace_outputs = traced(*args)\n",
    "#script_outputs = scripted(*args)\n",
    "\n",
    "\n",
    "assert torch.allclose(example_outputs[0], trace_outputs[0])\n",
    "assert torch.allclose(example_outputs[1], trace_outputs[1])\n",
    "\n",
    "print(\"trace and Module versions equivalent\")\n",
    "\n",
    "# assert torch.allclose(example_outputs[0], script_outputs[0])\n",
    "# assert torch.allclose(example_outputs[1], script_outputs[1])\n",
    "\n",
    "# print(\"script and Module versions equivalent\")\n",
    "\n",
    "# check they are equivalent with new values\n",
    "seq_len *= 2\n",
    "batch *= 2\n",
    "inp = torch.randn(batch, channels, in_features, seq_len)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,)).type(torch.int64)\n",
    "in_lens = in_lens.sort(descending=True)[0]\n",
    "\n",
    "args = (inp, in_lens)\n",
    "\n",
    "# expected outputs\n",
    "example_outputs = model(*args)\n",
    "trace_outputs = traced(*args)\n",
    "#script_outputs = scripted(*args)\n",
    "\n",
    "\n",
    "assert torch.allclose(example_outputs[0], trace_outputs[0], rtol=1e-2, atol=1e-4)\n",
    "assert torch.allclose(example_outputs[1], trace_outputs[1], rtol=1e-2, atol=1e-4)\n",
    "\n",
    "print(\"trace and Module versions equivalent for variable batch\")\n",
    "\n",
    "# assert torch.allclose(example_outputs[0], script_outputs[0], rtol=1e-2, atol=1e-4)\n",
    "# assert torch.allclose(example_outputs[1], script_outputs[1], rtol=1e-2, atol=1e-4)\n",
    "\n",
    "# print(\"script and Module versions equivalent for variable batch\")\n",
    "\n",
    "\n",
    "model = traced\n",
    "\n",
    "# export onnx model\n",
    "torch.onnx.export(model, args, fp, export_params=True, verbose=False,  example_outputs=example_outputs, \n",
    "                  dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names, \n",
    "                  opset_version=opset_version)\n",
    "\n",
    "    \n",
    "ort_session = export_and_check(model=traced, \n",
    "                 args = args,\n",
    "                 fname = onnx_fname,\n",
    "                 input_names = input_names,\n",
    "                 output_names = output_names,\n",
    "                 dynamic_axes = dynamic_axes,\n",
    "                 opset_version=11,\n",
    "                 example_outputs=example_outputs,\n",
    "    )\n",
    "\n",
    "\n",
    "batch = 10\n",
    "seq_len = 40\n",
    "\n",
    "inp = torch.randn(batch, channels, in_features, seq_len)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,))\n",
    "in_lens = in_lens.sort(descending=True)[0]\n",
    "\n",
    "compare_outputs(ort_session, model, args, output_names=output_names, input_names=input_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting to ONNX notes\n",
    "\n",
    "* No `Union` allowed in types\n",
    "* MAKE SURE cuda is disabled with `os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"` before exporting. \n",
    "* Make sure you place in .eval() mode before exporting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything below is an offcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # set this before importing torch\n",
    "import torch\n",
    "import onnx\n",
    "\n",
    "\n",
    "from myrtlespeech.protos import task_config_pb2\n",
    "from google.protobuf import text_format\n",
    "from myrtlespeech.builders.task_config import build\n",
    "from myrtlespeech.run.run import Saver\n",
    "from myrtlespeech.model.fully_connected import FullyConnected\n",
    "from myrtlespeech.model.rnn import RNN\n",
    "\n",
    "from myrtlespeech.run.train import fit\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import onnx\n",
    "\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/home/julian/exp/onnx/tmp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test to check identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_and_check(model, args, fname, input_names, output_names, example_outputs=None, \n",
    "                     dynamic_axes=None, verbose=False, opset_version=11):\n",
    "    fp = Path(log_dir) / fname\n",
    "    model.eval()\n",
    "    \n",
    "    # run model in torch to get expected outputs\n",
    "    exp_outputs = model(*args)\n",
    "    \n",
    "    # export onnx model\n",
    "    torch.onnx.export(model, args, fp, export_params=True, verbose=False,  example_outputs=example_outputs, \n",
    "                      dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names, \n",
    "                      opset_version=opset_version)\n",
    "    \n",
    "    \n",
    "    # Load the ONNX model\n",
    "    model_onnx = onnx.load(fp)\n",
    "    \n",
    "    # Print a human readable representation of the graph\n",
    "    if verbose:\n",
    "        print(\"Printing graph...\")\n",
    "        print(onnx.helper.printable_graph(model_onnx.graph))\n",
    "    \n",
    "    \n",
    "    # Check that the IR is well formed\n",
    "    #onnx.checker.check_model(model_onnx)\n",
    "    \n",
    "    \n",
    "    # onnx runtime\n",
    "    ort_session = ort.InferenceSession(str(fp))\n",
    "    \n",
    "    compare_outputs(ort_session, model, args, output_names, input_names)\n",
    "    \n",
    "    return ort_session\n",
    "\n",
    "def compare_outputs(ort_session, model, args, output_names, input_names):\n",
    "    # convert input args to numpy\n",
    "    exp_outputs = model(*args)\n",
    "    args = [x.numpy() if isinstance(x, torch.Tensor) else x for x in args]\n",
    "    \n",
    "    outputs = ort_session.run(output_names, {k: args[idx] for idx, k in enumerate(input_names)})\n",
    "    \n",
    "    check_outputs_as_expected(outputs, exp_outputs)\n",
    "    \n",
    "    print('model correct!')\n",
    "    \n",
    "\n",
    "    \n",
    "def check_outputs_as_expected(outputs, exp_outputs):\n",
    "    if isinstance(exp_outputs, torch.Tensor):\n",
    "        assert torch.allclose(torch.tensor(outputs), exp_outputs.cpu())\n",
    "    elif isinstance(exp_outputs, tuple) and isinstance(outputs, (tuple, list)):\n",
    "        assert len(exp_outputs) == len(outputs), f\"{len(exp_outputs)} != {len(outputs)}\"\n",
    "        for idx, x in enumerate(outputs):\n",
    "            check_outputs_as_expected(x, exp_outputs[idx])\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected output type(outputs)={type(outputs)} '\n",
    "                         f'with type(exp_outputs)={type(exp_outputs)} ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wrapper to unwrap tuple args\n",
    "class CollapseTupleArgs(torch.nn.Module):\n",
    "    def __init__(self, submodel):\n",
    "        super().__init__()\n",
    "        self.submodel = submodel\n",
    "    def forward(self, *args):\n",
    "        return self.submodel(args)\n",
    "\n",
    "class FlattenTupleArgs(torch.nn.Module):\n",
    "    \"\"\"Flatten Tuple Args before returning.\"\"\"\n",
    "    def __init__(self, submodel):\n",
    "        super().__init__()\n",
    "        self.submodel = submodel\n",
    "    def forward(self, *args):\n",
    "        res = self.submodel(*args)\n",
    "        ret = []\n",
    "        if isinstance(res, tuple):\n",
    "            for x in res:\n",
    "                if isinstance(x, tuple):\n",
    "                    for y in x:\n",
    "                        ret.append(y)\n",
    "                else:\n",
    "                    ret.append(x)\n",
    "        return tuple(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model correct!\n",
      "model correct!\n"
     ]
    }
   ],
   "source": [
    "model=torch.nn.Linear(2, 3)\n",
    "batch = 5\n",
    "args = (torch.randn(batch, 2),)\n",
    "exp_out = model(*args)\n",
    "model = torch.jit.trace(model, args)\n",
    "#model = torch.jit.script(model)\n",
    "ort_session = export_and_check(model=model, \n",
    "                 args = args,\n",
    "                 fname = 'linear.onnx',\n",
    "                 input_names = ['input'],\n",
    "                 output_names = ['output'],\n",
    "                 dynamic_axes = {'input': {0: 'batch'}, 'output': {0: 'batch'}},\n",
    "                 opset_version=11,\n",
    "                 example_outputs = exp_out,\n",
    "    )\n",
    "batch = 900\n",
    "compare_outputs(ort_session, model, (torch.randn(batch, 2),), ['output'], ['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Wrapper(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        return self.module(input, hx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN with trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace_output and nn.Module give same values\n",
      "trace_output and nn.Module give same values for new dimensions\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "log_dir = Path('/home/julian/exp/onnx/tmp/')\n",
    "\n",
    "class _Wrapper(torch.nn.Module):\n",
    "    \"\"\"Wrapper for to fix pytorch 1.4 bug.\"\"\"\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        return self.module(input, hx)\n",
    "\n",
    "# args\n",
    "seq_len = 1\n",
    "batch = 1\n",
    "input_size = 100\n",
    "num_layers = 8\n",
    "hidden_size = 300\n",
    "\n",
    "# args: (inp, h_n)\n",
    "# inp: (seq_len, batch, input_size)\n",
    "# h_n: (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "args = torch.randn(seq_len, batch, input_size), torch.randn((num_layers, batch, hidden_size))\n",
    "\n",
    "#onnx args\n",
    "input_names = ['input', 'hx']\n",
    "output_names = ['output', 'hy']\n",
    "dynamic_axes = {'input':  {0: 'seq_len', 1: 'batch'}, 'hx': {1: 'batch'},\n",
    "                'output': {0: 'seq_len', 1: 'batch'}, 'hy': {1: 'batch'},}\n",
    "\n",
    "model = torch.nn.GRU(input_size, hidden_size, num_layers)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "rnn_trace= torch.jit.trace(_Wrapper(model), args)\n",
    "\n",
    "example_outputs = model(*args) #check it runs\n",
    "trace_output = rnn_trace(*args)\n",
    "\n",
    "assert torch.allclose(example_outputs[0], trace_output[0])\n",
    "assert torch.allclose(example_outputs[1], trace_output[1])\n",
    "\n",
    "print(\"trace_output and nn.Module give same values\")\n",
    "\n",
    "torch.onnx.export(rnn_trace, args, log_dir / 'gru.onnx', export_params=True, verbose=False,  \n",
    "                  example_outputs=example_outputs, \n",
    "                  dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names, \n",
    "                  opset_version=11)\n",
    "\n",
    "\n",
    "## check for new args\n",
    "\n",
    "# args\n",
    "seq_len = 2000\n",
    "batch = 20\n",
    "\n",
    "\n",
    "# args: (inp, h_n)\n",
    "# inp: (seq_len, batch, input_size)\n",
    "# h_n: (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "args = torch.randn(seq_len, batch, input_size), torch.randn((num_layers, batch, hidden_size))\n",
    "\n",
    "example_outputs = model(*args) #check it runs\n",
    "trace_output = rnn_trace(*args)\n",
    "\n",
    "assert torch.allclose(example_outputs[0], trace_output[0])\n",
    "assert torch.allclose(example_outputs[1], trace_output[1])\n",
    "\n",
    "print(\"trace_output and nn.Module give same values for new dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN with script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script and nn.Module give same values\n",
      "graph(%input : Float(2, 1, 2),\n",
      "      %hx : Float(1, 1, 3),\n",
      "      %2 : Float(9, 2),\n",
      "      %3 : Float(9, 3),\n",
      "      %4 : Float(9),\n",
      "      %5 : Float(9)):\n",
      "  %6 : Tensor? = prim::Constant(), scope: __module.module\n",
      "  %7 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %8 : Tensor = onnx::Constant[value={3}](), scope: __module.module\n",
      "  %9 : Tensor = onnx::Constant[value={6}](), scope: __module.module\n",
      "  %10 : Tensor = onnx::Slice(%2, %8, %9, %7), scope: __module.module\n",
      "  %11 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %12 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %13 : Tensor = onnx::Constant[value={3}](), scope: __module.module\n",
      "  %14 : Tensor = onnx::Slice(%2, %12, %13, %11), scope: __module.module\n",
      "  %15 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %16 : Tensor = onnx::Constant[value={6}](), scope: __module.module\n",
      "  %17 : Tensor = onnx::Constant[value={9}](), scope: __module.module\n",
      "  %18 : Tensor = onnx::Slice(%2, %16, %17, %15), scope: __module.module\n",
      "  %19 : Tensor = onnx::Concat[axis=0](%10, %14, %18), scope: __module.module\n",
      "  %20 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %21 : Tensor = onnx::Constant[value={3}](), scope: __module.module\n",
      "  %22 : Tensor = onnx::Constant[value={6}](), scope: __module.module\n",
      "  %23 : Tensor = onnx::Slice(%3, %21, %22, %20), scope: __module.module\n",
      "  %24 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %25 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %26 : Tensor = onnx::Constant[value={3}](), scope: __module.module\n",
      "  %27 : Tensor = onnx::Slice(%3, %25, %26, %24), scope: __module.module\n",
      "  %28 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %29 : Tensor = onnx::Constant[value={6}](), scope: __module.module\n",
      "  %30 : Tensor = onnx::Constant[value={9}](), scope: __module.module\n",
      "  %31 : Tensor = onnx::Slice(%3, %29, %30, %28), scope: __module.module\n",
      "  %32 : Tensor = onnx::Concat[axis=0](%23, %27, %31), scope: __module.module\n",
      "  %33 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %34 : Tensor = onnx::Constant[value={3}](), scope: __module.module\n",
      "  %35 : Tensor = onnx::Constant[value={6}](), scope: __module.module\n",
      "  %36 : Tensor = onnx::Slice(%4, %34, %35, %33), scope: __module.module\n",
      "  %37 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %38 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %39 : Tensor = onnx::Constant[value={3}](), scope: __module.module\n",
      "  %40 : Tensor = onnx::Slice(%4, %38, %39, %37), scope: __module.module\n",
      "  %41 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %42 : Tensor = onnx::Constant[value={6}](), scope: __module.module\n",
      "  %43 : Tensor = onnx::Constant[value={9}](), scope: __module.module\n",
      "  %44 : Tensor = onnx::Slice(%4, %42, %43, %41), scope: __module.module\n",
      "  %45 : Tensor = onnx::Concat[axis=0](%36, %40, %44), scope: __module.module\n",
      "  %46 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %47 : Tensor = onnx::Constant[value={3}](), scope: __module.module\n",
      "  %48 : Tensor = onnx::Constant[value={6}](), scope: __module.module\n",
      "  %49 : Tensor = onnx::Slice(%5, %47, %48, %46), scope: __module.module\n",
      "  %50 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %51 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %52 : Tensor = onnx::Constant[value={3}](), scope: __module.module\n",
      "  %53 : Tensor = onnx::Slice(%5, %51, %52, %50), scope: __module.module\n",
      "  %54 : Tensor = onnx::Constant[value={0}](), scope: __module.module\n",
      "  %55 : Tensor = onnx::Constant[value={6}](), scope: __module.module\n",
      "  %56 : Tensor = onnx::Constant[value={9}](), scope: __module.module\n",
      "  %57 : Tensor = onnx::Slice(%5, %55, %56, %54), scope: __module.module\n",
      "  %58 : Tensor = onnx::Concat[axis=0](%49, %53, %57), scope: __module.module\n",
      "  %59 : Tensor = onnx::Concat[axis=0](%45, %58), scope: __module.module\n",
      "  %60 : Tensor = onnx::Unsqueeze[axes=[0]](%19), scope: __module.module\n",
      "  %61 : Tensor = onnx::Unsqueeze[axes=[0]](%32), scope: __module.module\n",
      "  %62 : Tensor = onnx::Unsqueeze[axes=[0]](%59), scope: __module.module\n",
      "  %63 : Tensor, %hy : Float(1, 1, 3) = onnx::GRU[hidden_size=3, linear_before_reset=1](%input, %60, %61, %62, %6, %hx), scope: __module.module # /home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/nn/modules/rnn.py:716:0\n",
      "  %output : Float(2, 1, 3) = onnx::Squeeze[axes=[1]](%63), scope: __module.module # /home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/nn/modules/rnn.py:716:0\n",
      "  return (%output, %hy)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "log_dir = Path('/home/julian/exp/onnx/tmp/')\n",
    "\n",
    "class _Wrapper(torch.nn.Module):\n",
    "    \"\"\"Wrapper for to fix pytorch 1.4 bug.\"\"\"\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        return self.module(input, hx)\n",
    "\n",
    "# args\n",
    "seq_len = 2\n",
    "batch = 1\n",
    "input_size = 2\n",
    "num_layers = 1\n",
    "hidden_size = 3\n",
    "\n",
    "# args: (inp, h_n)\n",
    "# inp: (seq_len, batch, input_size)\n",
    "# h_n: (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "args = torch.randn(seq_len, batch, input_size), torch.randn((num_layers, batch, hidden_size))\n",
    "\n",
    "#onnx args\n",
    "input_names = ['input', 'hx']\n",
    "output_names = ['output', 'hy']\n",
    "dynamic_axes = {'input':  {0: 'seq_len', 1: 'batch'}, 'hx': {1: 'batch'},\n",
    "                'output': {0: 'seq_len', 1: 'batch'}, 'hy': {1: 'batch'},}\n",
    "\n",
    "model = torch.nn.GRU(input_size, hidden_size, num_layers)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "rnn_script= torch.jit.trace(_Wrapper(model), args)\n",
    "\n",
    "example_outputs = model(*args) #check it runs\n",
    "script_output = rnn_script(*args)\n",
    "\n",
    "assert torch.allclose(example_outputs[0], script_output[0])\n",
    "assert torch.allclose(example_outputs[1], script_output[1])\n",
    "\n",
    "print(\"script and nn.Module give same values\")\n",
    "\n",
    "torch.onnx.export(rnn_script, args, log_dir / 'gru.onnx', export_params=True, verbose=True,  \n",
    "                  example_outputs=example_outputs, \n",
    "                  dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names, \n",
    "                  opset_version=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=_Wrapper\n",
      "  (module): RecursiveScriptModule(original_name=GRU)\n",
      ")\n",
      "RecursiveScriptModule(\n",
      "  original_name=_Wrapper\n",
      "  (module): RecursiveScriptModule(original_name=GRU)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.GRU(input_size, hidden_size, num_layers)\n",
    "\n",
    "example_outputs = model(*args)\n",
    "\n",
    "rnn_script= torch.jit.script(_Wrapper(model))\n",
    "rnn_script_= torch.jit.script(_Wrapper(torch.jit.script(model)))\n",
    "\n",
    "print(rnn_script)\n",
    "print(rnn_script_)\n",
    "assert torch.allclose(rnn_script(*args)[0], rnn_script_(*args)[0])\n",
    "assert torch.allclose(rnn_script(*args)[1], rnn_script_(*args)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0915, -0.0938,  0.0228],\n",
       "          [-0.1540, -0.1174,  0.0207],\n",
       "          [-0.0293, -0.0392,  0.0219]],\n",
       " \n",
       "         [[-0.1900, -0.1471,  0.0298],\n",
       "          [-0.1172, -0.1263,  0.0386],\n",
       "          [-0.1067, -0.1139,  0.0077]],\n",
       " \n",
       "         [[-0.2705, -0.1657,  0.0360],\n",
       "          [-0.1083, -0.1431,  0.0272],\n",
       "          [-0.1427, -0.1523,  0.0046]]], grad_fn=<StackBackward>),\n",
       " (tensor([[[-0.1653,  0.4739,  0.2383],\n",
       "           [ 0.2439,  0.1348,  0.1578],\n",
       "           [ 0.1438,  0.3261,  0.1405]],\n",
       "  \n",
       "          [[-0.2705, -0.1657,  0.0360],\n",
       "           [-0.1083, -0.1431,  0.0272],\n",
       "           [-0.1427, -0.1523,  0.0046]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[-0.2369,  0.6427,  0.5650],\n",
       "           [ 0.4649,  0.1760,  0.3083],\n",
       "           [ 0.3006,  0.4258,  0.2456]],\n",
       "  \n",
       "          [[-0.3988, -0.4197,  0.1099],\n",
       "           [-0.1745, -0.2874,  0.0790],\n",
       "           [-0.2203, -0.3272,  0.0133]]], grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 3\n",
    "num_layers = 2\n",
    "seq_len = 3\n",
    "batch = 3\n",
    "args = (torch.randn(seq_len, batch, input_size),)\n",
    "\n",
    "\n",
    "scripted_lstm = torch.jit.script(_Wrapper(torch.nn.LSTM(input_size, 3, num_layers)))\n",
    "\n",
    "scripted_lstm(*args)\n",
    "#model = torch.jit.trace(FlattenTupleArgs(scripted_lstm), args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## myrtlespeech submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CollapseTupleArgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-88971fb7c921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model = CollapseTupleArgs(FullyConnected(in_features, out_features=3, \n\u001b[0m\u001b[1;32m      8\u001b[0m                                     num_hidden_layers=2,  hidden_size = 2, hidden_activation_fn=torch.nn.ReLU()),)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CollapseTupleArgs' is not defined"
     ]
    }
   ],
   "source": [
    "# test myrtlespeech fully_connected\n",
    "# args: ([batch, seq_len, in_features], (batch,))\n",
    "\n",
    "seq_len = 2\n",
    "batch = 1\n",
    "in_features = 5\n",
    "model = CollapseTupleArgs(FullyConnected(in_features, out_features=3, \n",
    "                                    num_hidden_layers=2,  hidden_size = 2, hidden_activation_fn=torch.nn.ReLU()),)\n",
    "\n",
    "inp = torch.randn(batch, seq_len, in_features)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,))\n",
    "args = (inp, in_lens)\n",
    "\n",
    "model  = torch.jit.trace(model, args)\n",
    "\n",
    "output_names= ['output', 'out_lens']\n",
    "input_names=['input', 'in_lens']\n",
    "ort_session = export_and_check(model=model, \n",
    "                 args = args,\n",
    "                 fname = 'fc_myrtlespeech.onnx',\n",
    "                 input_names = input_names,\n",
    "                 output_names = output_names,\n",
    "                 dynamic_axes = {'input': {0: 'batch', 1: 'seq_len'}, \n",
    "                                 'in_lens': {0: 'batch'},\n",
    "                                 'output': {0: 'batch', 1: 'seq_len'}, \n",
    "                                 'out_lens': {0: 'batch'}},                        \n",
    "    )\n",
    "\n",
    "batch = 9\n",
    "seq_len = 4\n",
    "\n",
    "inp = torch.randn(batch, seq_len, in_features)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,))\n",
    "args = (inp, in_lens)\n",
    "compare_outputs(ort_session, model, args, output_names=output_names, input_names=input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script and Module versions equivalent\n",
      "sucessfully traced module\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/onnx/symbolic_opset9.py:1436: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable lenght with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  \"or define the initial states (h0/c0) as inputs of the model. \")\n"
     ]
    },
    {
     "ename": "Fail",
     "evalue": "[ONNXRuntimeError] : 1 : FAIL : Type Error: Type (tensor(int64)) of output arg (out_lens) of node () does not match expected type (tensor(int32)).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFail\u001b[0m                                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-bba06c056dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m                  \u001b[0mdynamic_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                  \u001b[0mopset_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                  \u001b[0mexample_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3809117e07fe>\u001b[0m in \u001b[0;36mexport_and_check\u001b[0;34m(model, args, fname, input_names, output_names, example_outputs, dynamic_axes, verbose, opset_version)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# onnx runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mort_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mcompare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mort_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/onnxruntime/capi/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path_or_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/onnxruntime/capi/session.py\u001b[0m in \u001b[0;36m_load_model\u001b[0;34m(self, providers)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unable to load from type '{0}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path_or_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFail\u001b[0m: [ONNXRuntimeError] : 1 : FAIL : Type Error: Type (tensor(int64)) of output arg (out_lens) of node () does not match expected type (tensor(int32))."
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Optional\n",
    "import torch \n",
    "from myrtlespeech.model.rnn import RNN\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "class FlattenRNNArgs(torch.nn.Module):\n",
    "    \"\"\"Flatten Tuple Args before returning and 'unflattens' at input.\"\"\"\n",
    "    def __init__(self, submodel):\n",
    "        super().__init__()\n",
    "        self.submodel = submodel\n",
    "    \n",
    "\n",
    "    def forward(self, *args):\n",
    "        # (inp, in_lens, hx) -> ((inp, in_lens), hx)\n",
    "        args_in = (args[0], args[1]), args[2]\n",
    "        res = self.submodel(*args_in)\n",
    "        ret = []\n",
    "        if isinstance(res, tuple):\n",
    "            for x in res:\n",
    "                if isinstance(x, tuple):\n",
    "                    for y in x:\n",
    "                        ret.append(y)\n",
    "                else:\n",
    "                    ret.append(x)\n",
    "        return tuple(ret)\n",
    "    \n",
    "# test myrtlespeech RNN\n",
    "# args: ([seq_len, batch, in_features], (batch,))\n",
    "\n",
    "\n",
    "seq_len = 2\n",
    "batch = 2\n",
    "input_size = 2\n",
    "num_layers = 1\n",
    "hidden_size = 3\n",
    "rnn_type = 1 # GRU\n",
    "# h_n = num_layers * num_directions, batch, hidden_size\n",
    "\n",
    "# inputs\n",
    "inp = torch.randn(seq_len, batch, input_size)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,)).type(torch.int32)\n",
    "in_lens = in_lens.sort(descending=True)[0]\n",
    "\n",
    "hx: Optional[torch.Tensor] = torch.randn(num_layers, batch, hidden_size)\n",
    "args = (inp, in_lens, hx)\n",
    "\n",
    "# init onnx params\n",
    "onnx_fname = 'RNN_myrtlespeech.onnx'\n",
    "input_names = ['input', 'in_lens', 'hx']\n",
    "output_names = ['output', 'out_lens', 'hx']\n",
    "dynamic_axes = {'input':  {0: 'seq_len', 1: 'batch'}, 'in_lens': {0: 'batch'}, 'hx': {1: 'batch'},\n",
    "                'output': {0: 'seq_len', 1: 'batch'}, 'out_lens': {0: 'batch'}, 'hx': {1: 'batch'},}\n",
    "\n",
    "# init model\n",
    "model = RNN(rnn_type=1, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "# script model\n",
    "scripted = torch.jit.script(model)\n",
    "\n",
    "# flatten tuple args\n",
    "model = FlattenRNNArgs(model)\n",
    "#scripted = torch.jit.trace(FlattenTupleArgs(scripted, unflatten_fn=unflatten_fn), args)\n",
    "scripted = FlattenRNNArgs(scripted)\n",
    "\n",
    "\n",
    "# expected outputs\n",
    "example_outputs = model(*args)\n",
    "script_outputs = scripted(*args)\n",
    "\n",
    "\n",
    "assert torch.allclose(example_outputs[0], script_outputs[0])\n",
    "assert torch.allclose(example_outputs[1], script_outputs[1])\n",
    "assert torch.allclose(example_outputs[2], script_outputs[2])\n",
    "\n",
    "print(\"script and Module versions equivalent\")\n",
    "\n",
    "traced = torch.jit.trace(model, args)\n",
    "\n",
    "print(\"sucessfully traced module\")\n",
    "\n",
    "\n",
    "ort_session = export_and_check(model=traced, \n",
    "                 args = args,\n",
    "                 fname = onnx_fname,\n",
    "                 input_names = input_names,\n",
    "                 output_names = output_names,\n",
    "                 dynamic_axes = dynamic_axes,\n",
    "                 opset_version=11,\n",
    "                 example_outputs=example_outputs,\n",
    "    )\n",
    "\n",
    "batch = 9\n",
    "seq_len = 4\n",
    "\n",
    "inp = torch.randn(batch, seq_len, in_features)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,))\n",
    "args = (inp, in_lens)\n",
    "compare_outputs(ort_session, model, args, output_names=output_names, input_names=input_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN notes\n",
    "* empty_like - not avavilable in torch 1.2. Hence upgrade to 1.4\n",
    "* TopK - not avavilable in most recent release. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myrtlespeech.builders.speech_to_text import build as build_stt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = CollapseTupleArgs(stt.model)\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepSpeech1(\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=494, out_features=8, bias=True)\n",
       "    (1): Hardtanh(min_val=0.0, max_val=20.0, inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (1): Hardtanh(min_val=0.0, max_val=20.0, inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (fc3): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (1): Hardtanh(min_val=0.0, max_val=20.0, inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (bi_lstm): LSTM(\n",
       "    (rnn): LSTM(16, 8, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (fc4): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (1): Hardtanh(min_val=0.0, max_val=20.0, inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (out): Linear(in_features=8, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Sequential\n",
       "  (0): RecursiveScriptModule(original_name=Linear)\n",
       "  (1): RecursiveScriptModule(original_name=Hardtanh)\n",
       "  (2): RecursiveScriptModule(original_name=Dropout)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted.submodel.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 correct\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "isTensor() INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1579040055865/work/aten/src/ATen/core/ivalue_inl.h:90, please report a bug to PyTorch. Expected Tensor but got Bool (toTensor at /opt/conda/conda-bld/pytorch_1579040055865/work/aten/src/ATen/core/ivalue_inl.h:90)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x47 (0x7f82941a2627 in /home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: torch::jit::LowerGraph(torch::jit::Graph&, c10::intrusive_ptr<c10::ivalue::Object, c10::detail::intrusive_target_default_null_type<c10::ivalue::Object> > const&) + 0x674 (0x7f82592b6794 in /home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/lib/libtorch.so)\nframe #2: <unknown function> + 0x6b6479 (0x7f8284b71479 in /home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #3: <unknown function> + 0x28ba06 (0x7f8284746a06 in /home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #4: _PyMethodDef_RawFastCallKeywords + 0x264 (0x5635de6446e4 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #5: _PyCFunction_FastCallKeywords + 0x21 (0x5635de644801 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #6: _PyEval_EvalFrameDefault + 0x4e8c (0x5635de6a02bc in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #7: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #8: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #9: _PyEval_EvalFrameDefault + 0x14ce (0x5635de69c8fe in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #10: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #11: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #12: _PyEval_EvalFrameDefault + 0x14ce (0x5635de69c8fe in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #13: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #14: _PyFunction_FastCallKeywords + 0x325 (0x5635de6439c5 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #15: _PyEval_EvalFrameDefault + 0x4aa9 (0x5635de69fed9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #16: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #17: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #18: _PyEval_EvalFrameDefault + 0x14ce (0x5635de69c8fe in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #19: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #20: PyEval_EvalCodeEx + 0x44 (0x5635de5e23c4 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #21: PyEval_EvalCode + 0x1c (0x5635de5e23ec in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #22: <unknown function> + 0x1e004d (0x5635de6ab04d in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #23: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x5635de644569 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #24: _PyCFunction_FastCallKeywords + 0x21 (0x5635de644801 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #25: _PyEval_EvalFrameDefault + 0x4755 (0x5635de69fb85 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #26: _PyGen_Send + 0x2a2 (0x5635de63d672 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #27: _PyEval_EvalFrameDefault + 0x1a6d (0x5635de69ce9d in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #28: _PyGen_Send + 0x2a2 (0x5635de63d672 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #29: _PyEval_EvalFrameDefault + 0x1a6d (0x5635de69ce9d in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #30: _PyGen_Send + 0x2a2 (0x5635de63d672 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #31: _PyMethodDef_RawFastCallKeywords + 0x8c (0x5635de64450c in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #32: _PyMethodDescr_FastCallKeywords + 0x4f (0x5635de64486f in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #33: _PyEval_EvalFrameDefault + 0x4c4c (0x5635de6a007c in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #34: _PyFunction_FastCallKeywords + 0xfb (0x5635de64379b in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #35: _PyEval_EvalFrameDefault + 0x416 (0x5635de69b846 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #36: _PyFunction_FastCallKeywords + 0xfb (0x5635de64379b in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #37: _PyEval_EvalFrameDefault + 0x6a0 (0x5635de69bad0 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #38: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #39: _PyFunction_FastCallDict + 0x400 (0x5635de5e2800 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #40: _PyObject_Call_Prepend + 0x63 (0x5635de5f9c43 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #41: PyObject_Call + 0x6e (0x5635de5ee95e in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #42: _PyEval_EvalFrameDefault + 0x1e20 (0x5635de69d250 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #43: _PyEval_EvalCodeWithName + 0x5da (0x5635de5e17da in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #44: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #45: _PyEval_EvalFrameDefault + 0x14ce (0x5635de69c8fe in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #46: <unknown function> + 0x171cc6 (0x5635de63ccc6 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #47: <unknown function> + 0x171ecb (0x5635de63cecb in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #48: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x5635de644569 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #49: _PyCFunction_FastCallKeywords + 0x21 (0x5635de644801 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #50: _PyEval_EvalFrameDefault + 0x4755 (0x5635de69fb85 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #51: _PyEval_EvalCodeWithName + 0x5da (0x5635de5e17da in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #52: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #53: _PyEval_EvalFrameDefault + 0x6a0 (0x5635de69bad0 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #54: <unknown function> + 0x171cc6 (0x5635de63ccc6 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #55: <unknown function> + 0x171ecb (0x5635de63cecb in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #56: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x5635de644569 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #57: _PyCFunction_FastCallKeywords + 0x21 (0x5635de644801 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #58: _PyEval_EvalFrameDefault + 0x4755 (0x5635de69fb85 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #59: _PyEval_EvalCodeWithName + 0x5da (0x5635de5e17da in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #60: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #61: _PyEval_EvalFrameDefault + 0x416 (0x5635de69b846 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #62: <unknown function> + 0x171cc6 (0x5635de63ccc6 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #63: <unknown function> + 0x171ecb (0x5635de63cecb in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-12060073f312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m torch.onnx.export(scripted.submodel.fc1, args, log_dir / 'ds1_fc1', export_params=True, verbose=True,  example_outputs=model.submodel.fc1(h), \n\u001b[1;32m     21\u001b[0m                   \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                   opset_version=opset_version)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs)\u001b[0m\n\u001b[1;32m    146\u001b[0m                         \u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_retain_param_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                         \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                         strip_doc_string, dynamic_axes, keep_initializers_as_inputs)\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0m_retain_param_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_retain_param_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mexample_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_doc_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_doc_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size)\u001b[0m\n\u001b[1;32m    414\u001b[0m                                                         \u001b[0mexample_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                                                         \u001b[0m_retain_param_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                                                         fixed_batch_size=fixed_batch_size)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;31m# TODO: Don't allocate a in-memory string for the protobuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mexample_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"example_outputs must be provided when exporting a ScriptModule\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mmethod_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lower_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0min_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             graph = _propagate_and_assign_input_shapes(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: isTensor() INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1579040055865/work/aten/src/ATen/core/ivalue_inl.h:90, please report a bug to PyTorch. Expected Tensor but got Bool (toTensor at /opt/conda/conda-bld/pytorch_1579040055865/work/aten/src/ATen/core/ivalue_inl.h:90)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x47 (0x7f82941a2627 in /home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: torch::jit::LowerGraph(torch::jit::Graph&, c10::intrusive_ptr<c10::ivalue::Object, c10::detail::intrusive_target_default_null_type<c10::ivalue::Object> > const&) + 0x674 (0x7f82592b6794 in /home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/lib/libtorch.so)\nframe #2: <unknown function> + 0x6b6479 (0x7f8284b71479 in /home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #3: <unknown function> + 0x28ba06 (0x7f8284746a06 in /home/julian/miniconda3/envs/myrtlespeech/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #4: _PyMethodDef_RawFastCallKeywords + 0x264 (0x5635de6446e4 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #5: _PyCFunction_FastCallKeywords + 0x21 (0x5635de644801 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #6: _PyEval_EvalFrameDefault + 0x4e8c (0x5635de6a02bc in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #7: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #8: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #9: _PyEval_EvalFrameDefault + 0x14ce (0x5635de69c8fe in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #10: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #11: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #12: _PyEval_EvalFrameDefault + 0x14ce (0x5635de69c8fe in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #13: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #14: _PyFunction_FastCallKeywords + 0x325 (0x5635de6439c5 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #15: _PyEval_EvalFrameDefault + 0x4aa9 (0x5635de69fed9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #16: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #17: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #18: _PyEval_EvalFrameDefault + 0x14ce (0x5635de69c8fe in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #19: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #20: PyEval_EvalCodeEx + 0x44 (0x5635de5e23c4 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #21: PyEval_EvalCode + 0x1c (0x5635de5e23ec in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #22: <unknown function> + 0x1e004d (0x5635de6ab04d in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #23: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x5635de644569 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #24: _PyCFunction_FastCallKeywords + 0x21 (0x5635de644801 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #25: _PyEval_EvalFrameDefault + 0x4755 (0x5635de69fb85 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #26: _PyGen_Send + 0x2a2 (0x5635de63d672 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #27: _PyEval_EvalFrameDefault + 0x1a6d (0x5635de69ce9d in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #28: _PyGen_Send + 0x2a2 (0x5635de63d672 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #29: _PyEval_EvalFrameDefault + 0x1a6d (0x5635de69ce9d in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #30: _PyGen_Send + 0x2a2 (0x5635de63d672 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #31: _PyMethodDef_RawFastCallKeywords + 0x8c (0x5635de64450c in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #32: _PyMethodDescr_FastCallKeywords + 0x4f (0x5635de64486f in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #33: _PyEval_EvalFrameDefault + 0x4c4c (0x5635de6a007c in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #34: _PyFunction_FastCallKeywords + 0xfb (0x5635de64379b in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #35: _PyEval_EvalFrameDefault + 0x416 (0x5635de69b846 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #36: _PyFunction_FastCallKeywords + 0xfb (0x5635de64379b in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #37: _PyEval_EvalFrameDefault + 0x6a0 (0x5635de69bad0 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #38: _PyEval_EvalCodeWithName + 0x2f9 (0x5635de5e14f9 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #39: _PyFunction_FastCallDict + 0x400 (0x5635de5e2800 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #40: _PyObject_Call_Prepend + 0x63 (0x5635de5f9c43 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #41: PyObject_Call + 0x6e (0x5635de5ee95e in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #42: _PyEval_EvalFrameDefault + 0x1e20 (0x5635de69d250 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #43: _PyEval_EvalCodeWithName + 0x5da (0x5635de5e17da in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #44: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #45: _PyEval_EvalFrameDefault + 0x14ce (0x5635de69c8fe in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #46: <unknown function> + 0x171cc6 (0x5635de63ccc6 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #47: <unknown function> + 0x171ecb (0x5635de63cecb in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #48: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x5635de644569 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #49: _PyCFunction_FastCallKeywords + 0x21 (0x5635de644801 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #50: _PyEval_EvalFrameDefault + 0x4755 (0x5635de69fb85 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #51: _PyEval_EvalCodeWithName + 0x5da (0x5635de5e17da in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #52: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #53: _PyEval_EvalFrameDefault + 0x6a0 (0x5635de69bad0 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #54: <unknown function> + 0x171cc6 (0x5635de63ccc6 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #55: <unknown function> + 0x171ecb (0x5635de63cecb in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #56: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x5635de644569 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #57: _PyCFunction_FastCallKeywords + 0x21 (0x5635de644801 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #58: _PyEval_EvalFrameDefault + 0x4755 (0x5635de69fb85 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #59: _PyEval_EvalCodeWithName + 0x5da (0x5635de5e17da in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #60: _PyFunction_FastCallKeywords + 0x387 (0x5635de643a27 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #61: _PyEval_EvalFrameDefault + 0x416 (0x5635de69b846 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #62: <unknown function> + 0x171cc6 (0x5635de63ccc6 in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\nframe #63: <unknown function> + 0x171ecb (0x5635de63cecb in /home/julian/miniconda3/envs/myrtlespeech/bin/python)\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(batch, channels, in_features, seq_len)\n",
    "in_lens = torch.randint(low=1, high=seq_len, size=(batch,)).type(torch.int64)\n",
    "in_lens = in_lens.sort(descending=True)[0]\n",
    "log_dir = Path(log_dir)\n",
    "args = (inp, in_lens)\n",
    "\n",
    "input_names = ['input',]\n",
    "output_names = ['output',]\n",
    "dynamic_axes = {'input': {0: 'batch', 1: 'seq_len'}, \n",
    "                     'output': {0: 'batch', 1: 'seq_len'}}\n",
    "opset_version = 11\n",
    "\n",
    "###\n",
    "h, seq_lens = args\n",
    "batch, channels, features, seq_len = h.size()\n",
    "h = h.view(batch, channels * features, seq_len).permute(0, 2, 1)\n",
    "\n",
    "assert torch.allclose(model.submodel.fc1(h), scripted.submodel.fc1(h))\n",
    "print(\"fc1 correct\")\n",
    "torch.onnx.export(scripted.submodel.fc1, args, log_dir / 'ds1_fc1', export_params=True, verbose=True,  example_outputs=model.submodel.fc1(h), \n",
    "                  dynamic_axes=dynamic_axes, input_names=input_names, output_names=output_names, \n",
    "                  opset_version=opset_version)\n",
    "\n",
    "\n",
    "h = model.submodel.fc1(h)\n",
    "assert torch.allclose(model.submodel.fc2(h), scripted.submodel.fc2(h))\n",
    "\n",
    "\n",
    "print(\"fc2 correct\")\n",
    "h = model.submodel.fc2(h)\n",
    "assert torch.allclose(model.submodel.fc3(h), scripted.submodel.fc3(h))\n",
    "\n",
    "print(\"fc3 correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.1624, 0.0000, 0.2714, 0.8505, 0.0000, 1.6649],\n",
       "         [0.1109, 0.0000, 0.0000, 0.8201, 0.3121, 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<DifferentiableGraphBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse example config file\n",
    "with open(\"../src/myrtlespeech/configs/rnn_t_en_2L.config\") as f:\n",
    "    task_config = text_format.Merge(f.read(), task_config_pb2.TaskConfig())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/home/julian/exp/rnnt/wer_down/2L/2/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all components for config\n",
    "# FYI: if using train-clean-100 & dev-clean this cell takes O(60s) \n",
    "seq_to_seq, epochs, train_loader, eval_loader = build(task_config, accumulation_steps=2)\n",
    "seq_to_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "epoch = 68\n",
    "training_state = {}\n",
    "if load_model:\n",
    "    fp = log_dir + f'state_dict_{epoch}.pt'\n",
    "    #fp = '/home/julian/exp/rnnt/wer_down/2D/1/model_saved.pt'\n",
    "    training_state = load_seq_to_seq(seq_to_seq, fp)\n",
    "    \n",
    "seq_to_seq.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
